{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SSL-GAN.ipynb",
      "provenance": [],
      "mount_file_id": "1f8XB7A5gQipyPHD5inHMqdb4ZDh_Qzlx",
      "authorship_tag": "ABX9TyM4d9IGbnqOp6qrQdMMpTSR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ayaanzhaque/APCSP-Create/blob/master/SSL_GAN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fxtKmRV3xFyH",
        "colab_type": "text"
      },
      "source": [
        "Tasks\n",
        "1. Train a Working GAN (done)\n",
        "2. Train a general classifer (Train the same classifer at different dataset sizes, plot its datasize vs accuracy)\n",
        "3. Train a classifier that uses the smaller dataset sizes + the generated images \n",
        "4. Export accuracy vs dataset size into a different file\n",
        "5. plot the data from the file, see the difference in accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XaW_Qyu0HZII",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import math\n",
        "import torch\n",
        "from torch import optim,nn\n",
        "import torchvision\n",
        "from torch.utils.data import DataLoader,Dataset\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.utils as vutils\n",
        "import pdb\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.io import loadmat\n",
        "from skimage import color\n",
        "from skimage import io\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "class DCGAN_generator(nn.Module):\n",
        "  \"\"\"\n",
        "\n",
        "  Attributes\n",
        "  ----------\n",
        "    ngpu : int\n",
        "      The number of available GPU devices\n",
        "\n",
        "  \"\"\"\n",
        "  def __init__(self, ngpu):\n",
        "    \"\"\"Init function\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "      ngpu : int\n",
        "        The number of available GPU devices\n",
        "\n",
        "    \"\"\"\n",
        "    super(DCGAN_generator, self).__init__()\n",
        "    self.ngpu = ngpu\n",
        "        \n",
        "    # just to test - will soon be args\n",
        "    nz = 100 # noise dimension\n",
        "    ngf = 64 # number of features map on the first layer\n",
        "    nc = 3 # number of channels\n",
        "\n",
        "    self.main = nn.Sequential(\n",
        "      # input is Z, going into a convolution\n",
        "      nn.ConvTranspose2d(     nz, ngf * 4, 4, 1, 0, bias=False),\n",
        "      nn.BatchNorm2d(ngf * 4),\n",
        "      nn.ReLU(True),\n",
        "      # state size. (ngf*8) x 4 x 4\n",
        "      nn.ConvTranspose2d(ngf * 4, ngf * 2, 4, 2, 1, bias=False),\n",
        "      nn.BatchNorm2d(ngf * 2),\n",
        "      nn.ReLU(True),\n",
        "      # state size. (ngf*4) x 8 x 8\n",
        "      nn.ConvTranspose2d(ngf * 2, ngf, 4, 2, 1, bias=False),\n",
        "      nn.BatchNorm2d(ngf),\n",
        "      nn.ReLU(True),\n",
        "      # state size. (ngf*2) x 16 x 16\n",
        "      nn.ConvTranspose2d(ngf, nc, 4, 2, 1, bias=False),\n",
        "      nn.Tanh()\n",
        "      # state size. (nc) x 64 x 64\n",
        "    )\n",
        "\n",
        "  def forward(self, input):\n",
        "    \"\"\"Forward function\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    input : :py:class:`torch.Tensor`\n",
        "    \n",
        "    Returns\n",
        "    -------\n",
        "    :py:class:`torch.Tensor`\n",
        "      the output of the generator (i.e. an image)\n",
        "\n",
        "    \"\"\"\n",
        "    #if isinstance(input.data, torch.cuda.FloatTensor) and self.ngpu > 1:\n",
        "    #  output = nn.parallel.data_parallel(self.main, input, range(self.ngpu))\n",
        "    #else:\n",
        "    #  output = self.main(input)\n",
        "    \n",
        "    # let's assume that we will never face the case where more than a GPU is used ...\n",
        "    output = self.main(input)\n",
        "    return output\n",
        "\n",
        "\n",
        "\n",
        "class DCGAN_discriminator(nn.Module):\n",
        "  \"\"\" \n",
        "\n",
        "  Attributes\n",
        "  ----------\n",
        "    ngpu : int\n",
        "      The number of available GPU devices\n",
        "\n",
        "  \"\"\"\n",
        "  def __init__(self, ngpu):\n",
        "    \"\"\"Init function\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "      ngpu : int\n",
        "        The number of available GPU devices\n",
        "\n",
        "    \"\"\"\n",
        "    super(DCGAN_discriminator, self).__init__()\n",
        "    self.ngpu = ngpu\n",
        "        \n",
        "        \n",
        "    # just to test - will soon be args\n",
        "    ndf = 64\n",
        "    nc = 3\n",
        "       \n",
        "    self.main = nn.Sequential(\n",
        "      nn.Conv2d(nc, ndf, 4, 2, 1, bias=False),\n",
        "      nn.BatchNorm2d(ndf),\n",
        "      nn.LeakyReLU(0.2, inplace=True),\n",
        "\n",
        "      nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False),\n",
        "      nn.BatchNorm2d(ndf * 2),\n",
        "      nn.LeakyReLU(0.2, inplace=True),\n",
        "      # state size. (ndf*4) x 8 x 8\n",
        "      nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False),\n",
        "      nn.BatchNorm2d(ndf * 4),\n",
        "      nn.LeakyReLU(0.2, inplace=True),\n",
        "      # state size. (ndf*8) x 4 x 4\n",
        "      nn.Conv2d(ndf * 4, 1, 4, 1, 0, bias=False),\n",
        "      nn.Sigmoid()\n",
        "    )\n",
        "\n",
        "  def forward(self, input):\n",
        "    \"\"\"Forward function\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    input : :py:class:`torch.Tensor`\n",
        "    \n",
        "    Returns\n",
        "    -------\n",
        "    :py:class:`torch.Tensor`\n",
        "      the output of the generator (i.e. an image)\n",
        "\n",
        "    \"\"\"\n",
        "    #if isinstance(input.data, torch.cuda.FloatTensor) and self.ngpu > 1:\n",
        "    #  output = nn.parallel.data_parallel(self.main, input, range(self.ngpu))\n",
        "    #else:\n",
        "    #  output = self.main(input)\n",
        "    \n",
        "    # let's assume that we will never face the case where more than a GPU is used ...\n",
        "    output = self.main(input)\n",
        "\n",
        "    return output.view(-1, 1).squeeze(1)\n",
        "\n",
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor()])\n",
        "\n",
        "batch_size = 64\n",
        "trainset = datasets.SVHN(\"/content\", split='train', download = True, transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
        "                                          shuffle=True, num_workers=2,)\n",
        "\n",
        "testset = datasets.SVHN(\"/content\", split='test', download = True, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
        "                                         shuffle=False, num_workers=2)\n",
        "\n",
        "# define device \n",
        "device = torch.device(\"cuda:0\")\n",
        "\n",
        "# data for plotting purposes\n",
        "generatorLosses = []\n",
        "discriminatorLosses = []\n",
        "\n",
        "#training starts\n",
        "\n",
        "epochs = 25\n",
        "\n",
        "input_size = 32\n",
        "\n",
        "real_label = 1\n",
        "fake_label = 0\n",
        "\n",
        "\n",
        "# models\n",
        "netG = DCGAN_generator(1)\n",
        "netD = DCGAN_discriminator(1)\n",
        "\n",
        "netG.to(device)\n",
        "netD.to(device)\n",
        "\n",
        "print(netG)\n",
        "\n",
        "# optimizers \n",
        "optD = optim.Adam(netD.parameters(), lr=0.0002, betas=(0.5, 0.999)) \n",
        "optG = optim.Adam(netG.parameters(), lr=0.0002, betas=(0.5, 0.999)) \n",
        "\n",
        "input_length = int(math.log(128, 2))\n",
        "\n",
        "loss = nn.BCELoss()\n",
        "\n",
        "for epoch in range(epochs):\n",
        "\n",
        "  for i, data in enumerate(trainloader, 0):\n",
        "    \n",
        "    dataiter = iter(trainloader)\n",
        "    inputs, labels = dataiter.next()\n",
        "    inputs, labels = inputs.to(device), labels.to(device)\n",
        "    tmpBatchSize = len(labels)\n",
        "\n",
        "    # create label arrays \n",
        "    true_label = torch.ones(tmpBatchSize, 1, device=device)\n",
        "    fake_label = torch.zeros(tmpBatchSize, 1, device=device)\n",
        "    # print(inputs)\n",
        "    # print(labels)\n",
        "\n",
        "    # generate fake images // im struggling here as well\n",
        "    r = torch.randn(tmpBatchSize, 100, 1, 1, device=device) #not sure if this is correct but it isnt giving errors\n",
        "    # print(r)\n",
        "    fakeImageBatch = netG(r)\n",
        "    # print(fakeImageBatch)\n",
        "\n",
        "    # # visualize the fake image \n",
        "    # plt.subplot(1,2,2)\n",
        "    # plt.axis(\"off\")\n",
        "    # plt.title(\"Fake Images\")\n",
        "    # plt.imshow(np.transpose(vutils.make_grid(fakeImageBatch, padding=2, normalize=True)))\n",
        "    # plt.show()\n",
        "\n",
        "    real_cpu = data[0].to(device)\n",
        "    batch_size = real_cpu.size(0)\n",
        "    # print(batch_size)\n",
        "\n",
        "    # train generator on real images\n",
        "    # predictionsReal = netD(real_cpu).view(-1)\n",
        "    predictionsReal = netD(inputs)\n",
        "    lossDiscriminator = loss(predictionsReal, true_label) #labels = 1\n",
        "    lossDiscriminator.backward(retain_graph = True)\n",
        "\n",
        "    # train generator on fake images\n",
        "    predictionsFake = netD(fakeImageBatch)\n",
        "    lossFake = loss(predictionsFake, fake_label)  #labels = 0\n",
        "    lossFake.backward(retain_graph= True)\n",
        "    optD.step() # update discriminator parameters    \n",
        "\n",
        "    # train generator \n",
        "    optG.zero_grad()\n",
        "    predictionsFake = netD(fakeImageBatch)\n",
        "    # batch_size = 8192\n",
        "    # true_label = torch.full((batch_size,), real_label, device=device)\n",
        "    lossGenerator = loss(predictionsFake, true_label) #labels = 1\n",
        "    lossGenerator.backward(retain_graph = True)\n",
        "    optG.step()\n",
        "\n",
        "    # reset the gradients\n",
        "    optD.zero_grad()\n",
        "    optG.zero_grad()\n",
        "\n",
        "    # save losses for graphing\n",
        "    generatorLosses.append(lossGenerator.item())\n",
        "    discriminatorLosses.append(lossDiscriminator.item())\n",
        "\n",
        "    # # save generated images \n",
        "    if(i % 100 == 0):\n",
        "       gridOfFakeImages = torchvision.utils.make_grid(fakeImageBatch.cpu())\n",
        "       torchvision.utils.save_image(gridOfFakeImages, \"/content/gridOfFakeImages/\" + str(epoch) + '_' + str(i) + '.png')\n",
        "\n",
        "  print(\"Epoch \" + str(epoch) + \"Complete\")\n",
        "  print(\"Generator Loss: \" + str(lossGenerator))\n",
        "  print(\"Discriminator Loss: \" + str(lossDiscriminator))\n",
        "\n",
        "def validate():\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for data in testloader:\n",
        "            images, labels = data\n",
        "            outputs = net(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    print('Accuracy of the network on the 10000 test images: %d %%' % (\n",
        "        100 * correct / total))\n",
        "\n",
        "#save models\n",
        "torch.save(netG, \"netG.h5\")\n",
        "torch.save(netD, \"netD.h5\")\n",
        "\n",
        "# plot losses\n",
        "plt.figure(figsize=(10,5))\n",
        "plt.title(\"Loss of Models\")\n",
        "plt.plot(generatorLosses,label=\"Generator\")\n",
        "plt.plot(discriminatorLosses,label=\"Discriminator\")\n",
        "plt.xlabel(\"Batches\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}