{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FixMatch.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMXLOuch8LNWI1eqlmi4Iu6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ayaanzhaque/APCSP-Create/blob/master/FixMatch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f1kdVROelfkV",
        "colab_type": "text"
      },
      "source": [
        "cifar.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "02OcSx8ok89d",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 368
        },
        "outputId": "0ae003d3-f6dd-4a1a-e4b9-7e813c4c79b1"
      },
      "source": [
        "import logging\n",
        "\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from torchvision import datasets\n",
        "from torchvision import transforms\n",
        "\n",
        "from .randaugment import RandAugmentMC\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "cifar10_mean = (0.4914, 0.4822, 0.4465)\n",
        "cifar10_std = (0.2471, 0.2435, 0.2616)\n",
        "cifar100_mean = (0.5071, 0.4867, 0.4408)\n",
        "cifar100_std = (0.2675, 0.2565, 0.2761)\n",
        "normal_mean = (0.5, 0.5, 0.5)\n",
        "normal_std = (0.5, 0.5, 0.5)\n",
        "\n",
        "\n",
        "def get_cifar10(root, num_labeled, num_expand_x, num_expand_u):\n",
        "    transform_labeled = transforms.Compose([\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.RandomCrop(size=32,\n",
        "                              padding=int(32*0.125),\n",
        "                              padding_mode='reflect'),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=cifar10_mean, std=cifar10_std)\n",
        "    ])\n",
        "    transform_val = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=cifar10_mean, std=cifar10_std)\n",
        "    ])\n",
        "    base_dataset = datasets.CIFAR10(root, train=True, download=True)\n",
        "\n",
        "    train_labeled_idxs, train_unlabeled_idxs = x_u_split(\n",
        "        base_dataset.targets, num_labeled, num_expand_x, num_expand_u, num_classes=10)\n",
        "\n",
        "    train_labeled_dataset = CIFAR10SSL(\n",
        "        root, train_labeled_idxs, train=True,\n",
        "        transform=transform_labeled)\n",
        "\n",
        "    train_unlabeled_dataset = CIFAR10SSL(\n",
        "        root, train_unlabeled_idxs, train=True,\n",
        "        transform=TransformFix(mean=cifar10_mean, std=cifar10_std))\n",
        "\n",
        "    test_dataset = datasets.CIFAR10(\n",
        "        root, train=False, transform=transform_val, download=False)\n",
        "    logger.info(\"Dataset: CIFAR10\")\n",
        "    logger.info(f\"Labeled examples: {len(train_labeled_idxs)}\"\n",
        "                f\" Unlabeled examples: {len(train_unlabeled_idxs)}\")\n",
        "\n",
        "    return train_labeled_dataset, train_unlabeled_dataset, test_dataset\n",
        "\n",
        "\n",
        "def get_cifar100(root, num_labeled, num_expand_x, num_expand_u):\n",
        "\n",
        "    transform_labeled = transforms.Compose([\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.RandomCrop(size=32,\n",
        "                              padding=int(32*0.125),\n",
        "                              padding_mode='reflect'),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=cifar100_mean, std=cifar100_std)])\n",
        "\n",
        "    transform_val = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=cifar100_mean, std=cifar100_std)])\n",
        "\n",
        "    base_dataset = datasets.CIFAR100(\n",
        "        root, train=True, download=True)\n",
        "\n",
        "    train_labeled_idxs, train_unlabeled_idxs =x_u_split(\n",
        "        base_dataset.targets, num_labeled, num_expand_x, num_expand_u, num_classes=100)\n",
        "\n",
        "    train_labeled_dataset = CIFAR100SSL(\n",
        "        root, train_labeled_idxs, train=True,\n",
        "        transform=transform_labeled)\n",
        "\n",
        "    train_unlabeled_dataset = CIFAR100SSL(\n",
        "        root, train_unlabeled_idxs, train=True,\n",
        "        transform=TransformFix(mean=cifar100_mean, std=cifar100_std))\n",
        "\n",
        "    test_dataset = datasets.CIFAR100(\n",
        "        root, train=False, transform=transform_val, download=False)\n",
        "\n",
        "    logger.info(\"Dataset: CIFAR100\")\n",
        "    logger.info(f\"Labeled examples: {len(train_labeled_idxs)}\"\n",
        "                f\" Unlabeled examples: {len(train_unlabeled_idxs)}\")\n",
        "\n",
        "    return train_labeled_dataset, train_unlabeled_dataset, test_dataset\n",
        "\n",
        "\n",
        "def x_u_split(labels,\n",
        "              num_labeled,\n",
        "              num_expand_x,\n",
        "              num_expand_u,\n",
        "              num_classes):\n",
        "    label_per_class = num_labeled // num_classes\n",
        "    labels = np.array(labels)\n",
        "    labeled_idx = []\n",
        "    unlabeled_idx = []\n",
        "    for i in range(num_classes):\n",
        "        idx = np.where(labels == i)[0]\n",
        "        np.random.shuffle(idx)\n",
        "        labeled_idx.extend(idx[:label_per_class])\n",
        "        unlabeled_idx.extend(idx[label_per_class:])\n",
        "\n",
        "    exapand_labeled = num_expand_x // len(labeled_idx)\n",
        "    exapand_unlabeled = num_expand_u // len(unlabeled_idx)\n",
        "    labeled_idx = np.hstack(\n",
        "        [labeled_idx for _ in range(exapand_labeled)])\n",
        "    unlabeled_idx = np.hstack(\n",
        "        [unlabeled_idx for _ in range(exapand_unlabeled)])\n",
        "\n",
        "    if len(labeled_idx) < num_expand_x:\n",
        "        diff = num_expand_x - len(labeled_idx)\n",
        "        labeled_idx = np.hstack(\n",
        "            (labeled_idx, np.random.choice(labeled_idx, diff)))\n",
        "    else:\n",
        "        assert len(labeled_idx) == num_expand_x\n",
        "\n",
        "    if len(unlabeled_idx) < num_expand_u:\n",
        "        diff = num_expand_u - len(unlabeled_idx)\n",
        "        unlabeled_idx = np.hstack(\n",
        "            (unlabeled_idx, np.random.choice(unlabeled_idx, diff)))\n",
        "    else:\n",
        "        assert len(unlabeled_idx) == num_expand_u\n",
        "\n",
        "    return labeled_idx, unlabeled_idx\n",
        "\n",
        "\n",
        "class TransformFix(object):\n",
        "    def __init__(self, mean, std):\n",
        "        self.weak = transforms.Compose([\n",
        "            transforms.RandomHorizontalFlip(),\n",
        "            transforms.RandomCrop(size=32,\n",
        "                                  padding=int(32*0.125),\n",
        "                                  padding_mode='reflect')])\n",
        "        self.strong = transforms.Compose([\n",
        "            transforms.RandomHorizontalFlip(),\n",
        "            transforms.RandomCrop(size=32,\n",
        "                                  padding=int(32*0.125),\n",
        "                                  padding_mode='reflect'),\n",
        "            RandAugmentMC(n=2, m=10)])\n",
        "        self.normalize = transforms.Compose([\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=mean, std=std)])\n",
        "\n",
        "    def __call__(self, x):\n",
        "        weak = self.weak(x)\n",
        "        strong = self.strong(x)\n",
        "        return self.normalize(weak), self.normalize(strong)\n",
        "\n",
        "\n",
        "class CIFAR10SSL(datasets.CIFAR10):\n",
        "    def __init__(self, root, indexs, train=True,\n",
        "                 transform=None, target_transform=None,\n",
        "                 download=False):\n",
        "        super().__init__(root, train=train,\n",
        "                         transform=transform,\n",
        "                         target_transform=target_transform,\n",
        "                         download=download)\n",
        "        if indexs is not None:\n",
        "            self.data = self.data[indexs]\n",
        "            self.targets = np.array(self.targets)[indexs]\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        img, target = self.data[index], self.targets[index]\n",
        "        img = Image.fromarray(img)\n",
        "\n",
        "        if self.transform is not None:\n",
        "            img = self.transform(img)\n",
        "\n",
        "        if self.target_transform is not None:\n",
        "            target = self.target_transform(target)\n",
        "\n",
        "        return img, target\n",
        "\n",
        "\n",
        "class CIFAR100SSL(datasets.CIFAR100):\n",
        "    def __init__(self, root, indexs, train=True,\n",
        "                 transform=None, target_transform=None,\n",
        "                 download=False):\n",
        "        super().__init__(root, train=train,\n",
        "                         transform=transform,\n",
        "                         target_transform=target_transform,\n",
        "                         download=download)\n",
        "        if indexs is not None:\n",
        "            self.data = self.data[indexs]\n",
        "            self.targets = np.array(self.targets)[indexs]\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        img, target = self.data[index], self.targets[index]\n",
        "        img = Image.fromarray(img)\n",
        "\n",
        "        if self.transform is not None:\n",
        "            img = self.transform(img)\n",
        "\n",
        "        if self.target_transform is not None:\n",
        "            target = self.target_transform(target)\n",
        "\n",
        "        return img, target\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-33dbdfd61bb4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchvision\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mrandaugment\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRandAugmentMC\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mlogger\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetLogger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named '__main__.randaugment'; '__main__' is not a package",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OYJoSO6HljkJ",
        "colab_type": "text"
      },
      "source": [
        "randaugment.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aeb4MHixldQM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import logging\n",
        "import random\n",
        "\n",
        "import numpy as np\n",
        "import PIL\n",
        "import PIL.ImageOps\n",
        "import PIL.ImageEnhance\n",
        "import PIL.ImageDraw\n",
        "from PIL import Image\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "PARAMETER_MAX = 10\n",
        "\n",
        "\n",
        "def AutoContrast(img, **kwarg):\n",
        "    return PIL.ImageOps.autocontrast(img)\n",
        "\n",
        "\n",
        "def Brightness(img, v, max_v, bias=0):\n",
        "    v = _float_parameter(v, max_v) + bias\n",
        "    return PIL.ImageEnhance.Brightness(img).enhance(v)\n",
        "\n",
        "\n",
        "def Color(img, v, max_v, bias=0):\n",
        "    v = _float_parameter(v, max_v) + bias\n",
        "    return PIL.ImageEnhance.Color(img).enhance(v)\n",
        "\n",
        "\n",
        "def Contrast(img, v, max_v, bias=0):\n",
        "    v = _float_parameter(v, max_v) + bias\n",
        "    return PIL.ImageEnhance.Contrast(img).enhance(v)\n",
        "\n",
        "\n",
        "def Cutout(img, v, max_v, bias=0):\n",
        "    if v == 0:\n",
        "        return img\n",
        "    v = _float_parameter(v, max_v) + bias\n",
        "    v = int(v * min(img.size))\n",
        "    return CutoutAbs(img, v)\n",
        "\n",
        "\n",
        "def CutoutAbs(img, v, **kwarg):\n",
        "    w, h = img.size\n",
        "    x0 = np.random.uniform(0, w)\n",
        "    y0 = np.random.uniform(0, h)\n",
        "    x0 = int(max(0, x0 - v / 2.))\n",
        "    y0 = int(max(0, y0 - v / 2.))\n",
        "    x1 = int(min(w, x0 + v))\n",
        "    y1 = int(min(h, y0 + v))\n",
        "    xy = (x0, y0, x1, y1)\n",
        "    # gray\n",
        "    color = (127, 127, 127)\n",
        "    img = img.copy()\n",
        "    PIL.ImageDraw.Draw(img).rectangle(xy, color)\n",
        "    return img\n",
        "\n",
        "\n",
        "def Equalize(img, **kwarg):\n",
        "    return PIL.ImageOps.equalize(img)\n",
        "\n",
        "\n",
        "def Identity(img, **kwarg):\n",
        "    return img\n",
        "\n",
        "\n",
        "def Invert(img, **kwarg):\n",
        "    return PIL.ImageOps.invert(img)\n",
        "\n",
        "\n",
        "def Posterize(img, v, max_v, bias=0):\n",
        "    v = _int_parameter(v, max_v) + bias\n",
        "    return PIL.ImageOps.posterize(img, v)\n",
        "\n",
        "\n",
        "def Rotate(img, v, max_v, bias=0):\n",
        "    v = _int_parameter(v, max_v) + bias\n",
        "    if random.random() < 0.5:\n",
        "        v = -v\n",
        "    return img.rotate(v)\n",
        "\n",
        "\n",
        "def Sharpness(img, v, max_v, bias=0):\n",
        "    v = _float_parameter(v, max_v) + bias\n",
        "    return PIL.ImageEnhance.Sharpness(img).enhance(v)\n",
        "\n",
        "\n",
        "def ShearX(img, v, max_v, bias=0):\n",
        "    v = _float_parameter(v, max_v) + bias\n",
        "    if random.random() < 0.5:\n",
        "        v = -v\n",
        "    return img.transform(img.size, PIL.Image.AFFINE, (1, v, 0, 0, 1, 0))\n",
        "\n",
        "\n",
        "def ShearY(img, v, max_v, bias=0):\n",
        "    v = _float_parameter(v, max_v) + bias\n",
        "    if random.random() < 0.5:\n",
        "        v = -v\n",
        "    return img.transform(img.size, PIL.Image.AFFINE, (1, 0, 0, v, 1, 0))\n",
        "\n",
        "\n",
        "def Solarize(img, v, max_v, bias=0):\n",
        "    v = _int_parameter(v, max_v) + bias\n",
        "    return PIL.ImageOps.solarize(img, 256 - v)\n",
        "\n",
        "\n",
        "def SolarizeAdd(img, v, max_v, bias=0, threshold=128):\n",
        "    v = _int_parameter(v, max_v) + bias\n",
        "    if random.random() < 0.5:\n",
        "        v = -v\n",
        "    img_np = np.array(img).astype(np.int)\n",
        "    img_np = img_np + v\n",
        "    img_np = np.clip(img_np, 0, 255)\n",
        "    img_np = img_np.astype(np.uint8)\n",
        "    img = Image.fromarray(img_np)\n",
        "    return PIL.ImageOps.solarize(img, threshold)\n",
        "\n",
        "\n",
        "def TranslateX(img, v, max_v, bias=0):\n",
        "    v = _float_parameter(v, max_v) + bias\n",
        "    if random.random() < 0.5:\n",
        "        v = -v\n",
        "    v = int(v * img.size[0])\n",
        "    return img.transform(img.size, PIL.Image.AFFINE, (1, 0, v, 0, 1, 0))\n",
        "\n",
        "\n",
        "def TranslateY(img, v, max_v, bias=0):\n",
        "    v = _float_parameter(v, max_v) + bias\n",
        "    if random.random() < 0.5:\n",
        "        v = -v\n",
        "    v = int(v * img.size[1])\n",
        "    return img.transform(img.size, PIL.Image.AFFINE, (1, 0, 0, 0, 1, v))\n",
        "\n",
        "\n",
        "def _float_parameter(v, max_v):\n",
        "    return float(v) * max_v / PARAMETER_MAX\n",
        "\n",
        "\n",
        "def _int_parameter(v, max_v):\n",
        "    return int(v * max_v / PARAMETER_MAX)\n",
        "\n",
        "\n",
        "def fixmatch_augment_pool():\n",
        "    # FixMatch paper\n",
        "    augs = [(AutoContrast, None, None),\n",
        "            (Brightness, 0.9, 0.05),\n",
        "            (Color, 0.9, 0.05),\n",
        "            (Contrast, 0.9, 0.05),\n",
        "            (Equalize, None, None),\n",
        "            (Identity, None, None),\n",
        "            (Posterize, 4, 4),\n",
        "            (Rotate, 30, 0),\n",
        "            (Sharpness, 0.9, 0.05),\n",
        "            (ShearX, 0.3, 0),\n",
        "            (ShearY, 0.3, 0),\n",
        "            (Solarize, 256, 0),\n",
        "            (TranslateX, 0.3, 0),\n",
        "            (TranslateY, 0.3, 0)]\n",
        "    return augs\n",
        "\n",
        "\n",
        "def my_augment_pool():\n",
        "    # Test\n",
        "    augs = [(AutoContrast, None, None),\n",
        "            (Brightness, 1.8, 0.1),\n",
        "            (Color, 1.8, 0.1),\n",
        "            (Contrast, 1.8, 0.1),\n",
        "            (Cutout, 0.2, 0),\n",
        "            (Equalize, None, None),\n",
        "            (Invert, None, None),\n",
        "            (Posterize, 4, 4),\n",
        "            (Rotate, 30, 0),\n",
        "            (Sharpness, 1.8, 0.1),\n",
        "            (ShearX, 0.3, 0),\n",
        "            (ShearY, 0.3, 0),\n",
        "            (Solarize, 256, 0),\n",
        "            (SolarizeAdd, 110, 0),\n",
        "            (TranslateX, 0.45, 0),\n",
        "            (TranslateY, 0.45, 0)]\n",
        "    return augs\n",
        "\n",
        "\n",
        "class RandAugmentPC(object):\n",
        "    def __init__(self, n, m):\n",
        "        assert n >= 1\n",
        "        assert 1 <= m <= 10\n",
        "        self.n = n\n",
        "        self.m = m\n",
        "        self.augment_pool = my_augment_pool()\n",
        "\n",
        "    def __call__(self, img):\n",
        "        ops = random.choices(self.augment_pool, k=self.n)\n",
        "        for op, max_v, bias in ops:\n",
        "            prob = np.random.uniform(0.2, 0.8)\n",
        "            if random.random() + prob >= 1:\n",
        "                img = op(img, v=self.m, max_v=max_v, bias=bias)\n",
        "        img = CutoutAbs(img, 16)\n",
        "        return img\n",
        "\n",
        "\n",
        "class RandAugmentMC(object):\n",
        "    def __init__(self, n, m):\n",
        "        assert n >= 1\n",
        "        assert 1 <= m <= 10\n",
        "        self.n = n\n",
        "        self.m = m\n",
        "        self.augment_pool = fixmatch_augment_pool()\n",
        "\n",
        "    def __call__(self, img):\n",
        "        ops = random.choices(self.augment_pool, k=self.n)\n",
        "        for op, max_v, bias in ops:\n",
        "            v = np.random.randint(1, self.m)\n",
        "            if random.random() < 0.5:\n",
        "                img = op(img, v=v, max_v=max_v, bias=bias)\n",
        "        img = CutoutAbs(img, 16)\n",
        "        return img\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hVNXploflxM9",
        "colab_type": "text"
      },
      "source": [
        "resnext.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V1edrMIolp29",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import logging\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "\n",
        "def mish(x):\n",
        "    \"\"\"Mish: A Self Regularized Non-Monotonic Neural Activation Function (https://arxiv.org/abs/1908.08681)\"\"\"\n",
        "    return x * torch.tanh(F.softplus(x))\n",
        "\n",
        "\n",
        "class PSBatchNorm2d(nn.BatchNorm2d):\n",
        "    \"\"\"How Does BN Increase Collapsed Neural Network Filters? (https://arxiv.org/abs/2001.11216)\"\"\"\n",
        "\n",
        "    def __init__(self, num_features, alpha=0.1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True):\n",
        "        super().__init__(num_features, eps, momentum, affine, track_running_stats)\n",
        "        self.alpha = alpha\n",
        "\n",
        "    def forward(self, x):\n",
        "        return super().forward(x) + self.alpha\n",
        "\n",
        "\n",
        "class ResNeXtBottleneck(nn.Module):\n",
        "    \"\"\"\n",
        "    RexNeXt bottleneck type C (https://github.com/facebookresearch/ResNeXt/blob/master/models/resnext.lua)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, stride,\n",
        "                 cardinality, base_width, widen_factor):\n",
        "        \"\"\" Constructor\n",
        "        Args:\n",
        "            in_channels: input channel dimensionality\n",
        "            out_channels: output channel dimensionality\n",
        "            stride: conv stride. Replaces pooling layer.\n",
        "            cardinality: num of convolution groups.\n",
        "            base_width: base number of channels in each group.\n",
        "            widen_factor: factor to reduce the input dimensionality before convolution.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        width_ratio = out_channels / (widen_factor * 64.)\n",
        "        D = cardinality * int(base_width * width_ratio)\n",
        "        self.conv_reduce = nn.Conv2d(\n",
        "            in_channels, D, kernel_size=1, stride=1, padding=0, bias=False)\n",
        "        self.bn_reduce = PSBatchNorm2d(D, momentum=0.001)\n",
        "        self.conv_conv = nn.Conv2d(D, D,\n",
        "                                   kernel_size=3, stride=stride, padding=1,\n",
        "                                   groups=cardinality, bias=False)\n",
        "        self.bn = PSBatchNorm2d(D, momentum=0.001)\n",
        "        self.act = mish\n",
        "        self.conv_expand = nn.Conv2d(\n",
        "            D, out_channels, kernel_size=1, stride=1, padding=0, bias=False)\n",
        "        self.bn_expand = PSBatchNorm2d(out_channels, momentum=0.001)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if in_channels != out_channels:\n",
        "            self.shortcut.add_module('shortcut_conv',\n",
        "                                     nn.Conv2d(in_channels, out_channels,\n",
        "                                               kernel_size=1,\n",
        "                                               stride=stride,\n",
        "                                               padding=0,\n",
        "                                               bias=False))\n",
        "            self.shortcut.add_module(\n",
        "                'shortcut_bn', PSBatchNorm2d(out_channels, momentum=0.001))\n",
        "\n",
        "    def forward(self, x):\n",
        "        bottleneck = self.conv_reduce.forward(x)\n",
        "        bottleneck = self.act(self.bn_reduce.forward(bottleneck))\n",
        "        bottleneck = self.conv_conv.forward(bottleneck)\n",
        "        bottleneck = self.act(self.bn.forward(bottleneck))\n",
        "        bottleneck = self.conv_expand.forward(bottleneck)\n",
        "        bottleneck = self.bn_expand.forward(bottleneck)\n",
        "        residual = self.shortcut.forward(x)\n",
        "        return self.act(residual + bottleneck)\n",
        "\n",
        "\n",
        "class CifarResNeXt(nn.Module):\n",
        "    \"\"\"\n",
        "    ResNext optimized for the Cifar dataset, as specified in\n",
        "    https://arxiv.org/pdf/1611.05431.pdf\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, cardinality, depth, num_classes,\n",
        "                 base_width, widen_factor=4):\n",
        "        \"\"\" Constructor\n",
        "        Args:\n",
        "            cardinality: number of convolution groups.\n",
        "            depth: number of layers.\n",
        "            nlabels: number of classes\n",
        "            base_width: base number of channels in each group.\n",
        "            widen_factor: factor to adjust the channel dimensionality\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.cardinality = cardinality\n",
        "        self.depth = depth\n",
        "        self.block_depth = (self.depth - 2) // 9\n",
        "        self.base_width = base_width\n",
        "        self.widen_factor = widen_factor\n",
        "        self.nlabels = num_classes\n",
        "        self.output_size = 64\n",
        "        self.stages = [64, 64 * self.widen_factor, 128 *\n",
        "                       self.widen_factor, 256 * self.widen_factor]\n",
        "\n",
        "        self.conv_1_3x3 = nn.Conv2d(3, 64, 3, 1, 1, bias=False)\n",
        "        self.bn_1 = PSBatchNorm2d(64, momentum=0.001)\n",
        "        self.act = mish\n",
        "        self.stage_1 = self.block('stage_1', self.stages[0], self.stages[1], 1)\n",
        "        self.stage_2 = self.block('stage_2', self.stages[1], self.stages[2], 2)\n",
        "        self.stage_3 = self.block('stage_3', self.stages[2], self.stages[3], 2)\n",
        "        self.classifier = nn.Linear(self.stages[3], num_classes)\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight,\n",
        "                                        mode='fan_out',\n",
        "                                        nonlinearity='leaky_relu')\n",
        "            elif isinstance(m, PSBatchNorm2d):\n",
        "                nn.init.constant_(m.weight, 1.0)\n",
        "                nn.init.constant_(m.bias, 0.0)\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                nn.init.xavier_normal_(m.weight)\n",
        "                nn.init.constant_(m.bias, 0.0)\n",
        "\n",
        "    def block(self, name, in_channels, out_channels, pool_stride=2):\n",
        "        \"\"\" Stack n bottleneck modules where n is inferred from the depth of the network.\n",
        "        Args:\n",
        "            name: string name of the current block.\n",
        "            in_channels: number of input channels\n",
        "            out_channels: number of output channels\n",
        "            pool_stride: factor to reduce the spatial dimensionality in the first bottleneck of the block.\n",
        "        Returns: a Module consisting of n sequential bottlenecks.\n",
        "        \"\"\"\n",
        "        block = nn.Sequential()\n",
        "        for bottleneck in range(self.block_depth):\n",
        "            name_ = '%s_bottleneck_%d' % (name, bottleneck)\n",
        "            if bottleneck == 0:\n",
        "                block.add_module(name_, ResNeXtBottleneck(in_channels,\n",
        "                                                          out_channels,\n",
        "                                                          pool_stride,\n",
        "                                                          self.cardinality,\n",
        "                                                          self.base_width,\n",
        "                                                          self.widen_factor))\n",
        "            else:\n",
        "                block.add_module(name_,\n",
        "                                 ResNeXtBottleneck(out_channels,\n",
        "                                                   out_channels,\n",
        "                                                   1,\n",
        "                                                   self.cardinality,\n",
        "                                                   self.base_width,\n",
        "                                                   self.widen_factor))\n",
        "        return block\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv_1_3x3.forward(x)\n",
        "        x = self.act(self.bn_1.forward(x))\n",
        "        x = self.stage_1.forward(x)\n",
        "        x = self.stage_2.forward(x)\n",
        "        x = self.stage_3.forward(x)\n",
        "        x = F.adaptive_avg_pool2d(x, 1)\n",
        "        x = x.view(-1, self.stages[3])\n",
        "        return self.classifier(x)\n",
        "\n",
        "\n",
        "def build_resnext(cardinality, depth, width, num_classes):\n",
        "    logger.info(f\"Model: ResNeXt {depth+1}x{width}\")\n",
        "    return CifarResNeXt(cardinality=cardinality,\n",
        "                        depth=depth,\n",
        "                        base_width=width,\n",
        "                        num_classes=num_classes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XL9h_627l4xb",
        "colab_type": "text"
      },
      "source": [
        "wideresnet.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rp-j1gOBl1ZY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import logging\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "\n",
        "def mish(x):\n",
        "    \"\"\"Mish: A Self Regularized Non-Monotonic Neural Activation Function (https://arxiv.org/abs/1908.08681)\"\"\"\n",
        "    return x * torch.tanh(F.softplus(x))\n",
        "\n",
        "\n",
        "class PSBatchNorm2d(nn.BatchNorm2d):\n",
        "    \"\"\"How Does BN Increase Collapsed Neural Network Filters? (https://arxiv.org/abs/2001.11216)\"\"\"\n",
        "\n",
        "    def __init__(self, num_features, alpha=0.1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True):\n",
        "        super().__init__(num_features, eps, momentum, affine, track_running_stats)\n",
        "        self.alpha = alpha\n",
        "\n",
        "    def forward(self, x):\n",
        "        return super().forward(x) + self.alpha\n",
        "\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    def __init__(self, in_planes, out_planes, stride, drop_rate=0.0, activate_before_residual=False):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.bn1 = PSBatchNorm2d(in_planes, momentum=0.001)\n",
        "        self.relu1 = nn.LeakyReLU(negative_slope=0.1, inplace=True)\n",
        "        self.conv1 = nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
        "                               padding=1, bias=False)\n",
        "        self.bn2 = PSBatchNorm2d(out_planes, momentum=0.001)\n",
        "        self.relu2 = nn.LeakyReLU(negative_slope=0.1, inplace=True)\n",
        "        self.conv2 = nn.Conv2d(out_planes, out_planes, kernel_size=3, stride=1,\n",
        "                               padding=1, bias=False)\n",
        "        self.drop_rate = drop_rate\n",
        "        self.equalInOut = (in_planes == out_planes)\n",
        "        self.convShortcut = (not self.equalInOut) and nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride,\n",
        "                                                                padding=0, bias=False) or None\n",
        "        self.activate_before_residual = activate_before_residual\n",
        "\n",
        "    def forward(self, x):\n",
        "        if not self.equalInOut and self.activate_before_residual == True:\n",
        "            x = self.relu1(self.bn1(x))\n",
        "        else:\n",
        "            out = self.relu1(self.bn1(x))\n",
        "        out = self.relu2(self.bn2(self.conv1(out if self.equalInOut else x)))\n",
        "        if self.drop_rate > 0:\n",
        "            out = F.dropout(out, p=self.drop_rate, training=self.training)\n",
        "        out = self.conv2(out)\n",
        "        return torch.add(x if self.equalInOut else self.convShortcut(x), out)\n",
        "\n",
        "\n",
        "class NetworkBlock(nn.Module):\n",
        "    def __init__(self, nb_layers, in_planes, out_planes, block, stride, drop_rate=0.0, activate_before_residual=False):\n",
        "        super(NetworkBlock, self).__init__()\n",
        "        self.layer = self._make_layer(\n",
        "            block, in_planes, out_planes, nb_layers, stride, drop_rate, activate_before_residual)\n",
        "\n",
        "    def _make_layer(self, block, in_planes, out_planes, nb_layers, stride, drop_rate, activate_before_residual):\n",
        "        layers = []\n",
        "        for i in range(int(nb_layers)):\n",
        "            layers.append(block(i == 0 and in_planes or out_planes, out_planes,\n",
        "                                i == 0 and stride or 1, drop_rate, activate_before_residual))\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.layer(x)\n",
        "\n",
        "\n",
        "class WideResNet(nn.Module):\n",
        "    def __init__(self, num_classes, depth=28, widen_factor=2, drop_rate=0.0):\n",
        "        super(WideResNet, self).__init__()\n",
        "        channels = [16, 16*widen_factor, 32*widen_factor, 64*widen_factor]\n",
        "        assert((depth - 4) % 6 == 0)\n",
        "        n = (depth - 4) / 6\n",
        "        block = BasicBlock\n",
        "        # 1st conv before any network block\n",
        "        self.conv1 = nn.Conv2d(3, channels[0], kernel_size=3, stride=1,\n",
        "                               padding=1, bias=False)\n",
        "        # 1st block\n",
        "        self.block1 = NetworkBlock(\n",
        "            n, channels[0], channels[1], block, 1, drop_rate, activate_before_residual=True)\n",
        "        # 2nd block\n",
        "        self.block2 = NetworkBlock(\n",
        "            n, channels[1], channels[2], block, 2, drop_rate)\n",
        "        # 3rd block\n",
        "        self.block3 = NetworkBlock(\n",
        "            n, channels[2], channels[3], block, 2, drop_rate)\n",
        "        # global average pooling and classifier\n",
        "        self.bn1 = PSBatchNorm2d(channels[3], momentum=0.001)\n",
        "        self.relu = nn.LeakyReLU(negative_slope=0.1, inplace=True)\n",
        "        self.fc = nn.Linear(channels[3], num_classes)\n",
        "        self.channels = channels[3]\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight,\n",
        "                                        mode='fan_out',\n",
        "                                        nonlinearity='leaky_relu')\n",
        "            elif isinstance(m, PSBatchNorm2d):\n",
        "                nn.init.constant_(m.weight, 1.0)\n",
        "                nn.init.constant_(m.bias, 0.0)\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                nn.init.xavier_normal_(m.weight)\n",
        "                nn.init.constant_(m.bias, 0.0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.conv1(x)\n",
        "        out = self.block1(out)\n",
        "        out = self.block2(out)\n",
        "        out = self.block3(out)\n",
        "        out = self.relu(self.bn1(out))\n",
        "        out = F.adaptive_avg_pool2d(out, 1)\n",
        "        out = out.view(-1, self.channels)\n",
        "        return self.fc(out)\n",
        "\n",
        "\n",
        "def build_wideresnet(depth, widen_factor, dropout, num_classes):\n",
        "    logger.info(f\"Model: WideResNet {depth}x{widen_factor}\")\n",
        "    return WideResNet(depth=depth,\n",
        "                      widen_factor=widen_factor,\n",
        "                      drop_rate=dropout,\n",
        "                      num_classes=num_classes)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LWf38t4ImM8i",
        "colab_type": "text"
      },
      "source": [
        "train.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "98QxvO-4mNoU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 368
        },
        "outputId": "cf5d5d3e-d747-47a4-aa93-83629cd8f925"
      },
      "source": [
        "import argparse\n",
        "import logging\n",
        "import math\n",
        "import os\n",
        "import random\n",
        "import shutil\n",
        "import time\n",
        "from copy import deepcopy\n",
        "from collections import OrderedDict\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.backends.cudnn as cudnn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import LambdaLR\n",
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
        "from torch.utils.data.distributed import DistributedSampler\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from tqdm import tqdm\n",
        "\n",
        "from dataset.cifar import get_cifar10, get_cifar100\n",
        "from utils import AverageMeter, accuracy\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "DATASET_GETTERS = {'cifar10': get_cifar10,\n",
        "                   'cifar100': get_cifar100}\n",
        "best_acc = 0\n",
        "\n",
        "\n",
        "def save_checkpoint(state, is_best, checkpoint, filename='checkpoint.pth.tar'):\n",
        "    filepath = os.path.join(checkpoint, filename)\n",
        "    torch.save(state, filepath)\n",
        "    if is_best:\n",
        "        shutil.copyfile(filepath, os.path.join(checkpoint,\n",
        "                                               'model_best.pth.tar'))\n",
        "\n",
        "\n",
        "def set_seed(args):\n",
        "    random.seed(args.seed)\n",
        "    np.random.seed(args.seed)\n",
        "    torch.manual_seed(args.seed)\n",
        "    torch.cuda.manual_seed(args.seed)\n",
        "    if args.n_gpu > 0:\n",
        "        torch.cuda.manual_seed_all(args.seed)\n",
        "\n",
        "\n",
        "def get_cosine_schedule_with_warmup(optimizer,\n",
        "                                    num_warmup_steps,\n",
        "                                    num_training_steps,\n",
        "                                    num_cycles=7./16.,\n",
        "                                    last_epoch=-1):\n",
        "    def _lr_lambda(current_step):\n",
        "        if current_step < num_warmup_steps:\n",
        "            return float(current_step) / float(max(1, num_warmup_steps))\n",
        "        no_progress = float(current_step - num_warmup_steps) / \\\n",
        "            float(max(1, num_training_steps - num_warmup_steps))\n",
        "        return max(0., math.cos(math.pi * num_cycles * no_progress))\n",
        "\n",
        "    return LambdaLR(optimizer, _lr_lambda, last_epoch)\n",
        "\n",
        "\n",
        "def main():\n",
        "    parser = argparse.ArgumentParser(description='PyTorch FixMatch Training')\n",
        "    parser.add_argument('--gpu-id', default='0', type=int,\n",
        "                        help='id(s) for CUDA_VISIBLE_DEVICES')\n",
        "    parser.add_argument('--num-workers', type=int, default=4,\n",
        "                        help='number of workers')\n",
        "    parser.add_argument('--dataset', default='cifar10', type=str,\n",
        "                        choices=['cifar10', 'cifar100'],\n",
        "                        help='dataset name')\n",
        "    parser.add_argument('--num-labeled', type=int, default=4000,\n",
        "                        help='number of labeled data')\n",
        "    parser.add_argument('--arch', default='wideresnet', type=str,\n",
        "                        choices=['wideresnet', 'resnext'],\n",
        "                        help='dataset name')\n",
        "    parser.add_argument('--epochs', default=100, type=int,\n",
        "                        help='number of total epochs to run')\n",
        "    parser.add_argument('--start-epoch', default=0, type=int,\n",
        "                        help='manual epoch number (useful on restarts)')\n",
        "    parser.add_argument('--batch-size', default=64, type=int,\n",
        "                        help='train batchsize')\n",
        "    parser.add_argument('--lr', '--learning-rate', default=0.03, type=float,\n",
        "                        help='initial learning rate')\n",
        "    parser.add_argument('--warmup', default=0, type=float,\n",
        "                        help='warmup epochs (unlabeled data based)')\n",
        "    parser.add_argument('--wdecay', default=5e-4, type=float,\n",
        "                        help='weight decay')\n",
        "    parser.add_argument('--nesterov', action='store_true', default=True,\n",
        "                        help='use nesterov momentum')\n",
        "    parser.add_argument('--use-ema', action='store_true', default=True,\n",
        "                        help='use EMA model')\n",
        "    parser.add_argument('--ema-decay', default=0.999, type=float,\n",
        "                        help='EMA decay rate')\n",
        "    parser.add_argument('--mu', default=7, type=int,\n",
        "                        help='coefficient of unlabeled batch size')\n",
        "    parser.add_argument('--lambda-u', default=1, type=float,\n",
        "                        help='coefficient of unlabeled loss')\n",
        "    parser.add_argument('--threshold', default=0.95, type=float,\n",
        "                        help='pseudo label threshold')\n",
        "    parser.add_argument('--k-img', default=65536, type=int,\n",
        "                        help='number of labeled examples')\n",
        "    parser.add_argument('--out', default='result',\n",
        "                        help='directory to output the result')\n",
        "    parser.add_argument('--resume', default='', type=str,\n",
        "                        help='path to latest checkpoint (default: none)')\n",
        "    parser.add_argument('--seed', type=int, default=-1,\n",
        "                        help=\"random seed (-1: don't use random seed)\")\n",
        "    parser.add_argument(\"--amp\", action=\"store_true\",\n",
        "                        help=\"use 16-bit (mixed) precision through NVIDIA apex AMP\")\n",
        "    parser.add_argument(\"--opt_level\", type=str, default=\"O1\",\n",
        "                        help=\"apex AMP optimization level selected in ['O0', 'O1', 'O2', and 'O3'].\"\n",
        "                        \"See details at https://nvidia.github.io/apex/amp.html\")\n",
        "    parser.add_argument(\"--local_rank\", type=int, default=-1,\n",
        "                        help=\"For distributed training: local_rank\")\n",
        "    parser.add_argument('--no-progress', action='store_true',\n",
        "                        help=\"don't use progress bar\")\n",
        "\n",
        "    args = parser.parse_args()\n",
        "    global best_acc\n",
        "\n",
        "    def create_model(args):\n",
        "        if args.arch == 'wideresnet':\n",
        "            import models.wideresnet as models\n",
        "            model = models.build_wideresnet(depth=args.model_depth,\n",
        "                                            widen_factor=args.model_width,\n",
        "                                            dropout=0,\n",
        "                                            num_classes=args.num_classes)\n",
        "        elif args.arch == 'resnext':\n",
        "            import models.resnext as models\n",
        "            model = models.build_resnext(cardinality=args.model_cardinality,\n",
        "                                         depth=args.model_depth,\n",
        "                                         width=args.model_width,\n",
        "                                         num_classes=args.num_classes)\n",
        "\n",
        "        logger.info(\"Total params: {:.2f}M\".format(\n",
        "            sum(p.numel() for p in model.parameters())/1e6))\n",
        "\n",
        "        return model\n",
        "\n",
        "    if args.local_rank == -1:\n",
        "        device = torch.device('cuda', args.gpu_id)\n",
        "        args.world_size = 1\n",
        "        args.n_gpu = torch.cuda.device_count()\n",
        "    else:\n",
        "        torch.cuda.set_device(args.local_rank)\n",
        "        device = torch.device('cuda', args.local_rank)\n",
        "        torch.distributed.init_process_group(backend='nccl')\n",
        "        args.world_size = torch.distributed.get_world_size()\n",
        "        args.n_gpu = 1\n",
        "\n",
        "    args.device = device\n",
        "\n",
        "    if args.dataset == 'cifar10':\n",
        "        args.num_classes = 10\n",
        "        if args.arch == 'wideresnet':\n",
        "            args.model_depth = 28\n",
        "            args.model_width = 2\n",
        "        if args.arch == 'resnext':\n",
        "            args.model_cardinality = 4\n",
        "            args.model_depth = 28\n",
        "            args.model_width = 4\n",
        "\n",
        "    elif args.dataset == 'cifar100':\n",
        "        args.num_classes = 100\n",
        "        if args.arch == 'wideresnet':\n",
        "            args.model_depth = 28\n",
        "            args.model_width = 10\n",
        "        if args.arch == 'resnext':\n",
        "            args.model_cardinality = 8\n",
        "            args.model_depth = 29\n",
        "            args.model_width = 64\n",
        "\n",
        "    logging.basicConfig(\n",
        "        format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n",
        "        datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
        "        level=logging.INFO if args.local_rank in [-1, 0] else logging.WARN)\n",
        "\n",
        "    logger.warning(\n",
        "        f\"Process rank: {args.local_rank}, \"\n",
        "        f\"device: {args.device}, \"\n",
        "        f\"n_gpu: {args.n_gpu}, \"\n",
        "        f\"distributed training: {bool(args.local_rank != -1)}, \"\n",
        "        f\"16-bits training: {args.amp}\",)\n",
        "\n",
        "    logger.info(dict(args._get_kwargs()))\n",
        "\n",
        "    if args.seed != -1:\n",
        "        set_seed(args)\n",
        "\n",
        "    if args.local_rank in [-1, 0]:\n",
        "        os.makedirs(args.out, exist_ok=True)\n",
        "        writer = SummaryWriter(args.out)\n",
        "\n",
        "    if args.local_rank not in [-1, 0]:\n",
        "        torch.distributed.barrier()\n",
        "\n",
        "    labeled_dataset, unlabeled_dataset, test_dataset = DATASET_GETTERS[args.dataset](\n",
        "        './data', args.num_labeled, args.k_img, args.k_img * args.mu)\n",
        "\n",
        "    model = create_model(args)\n",
        "\n",
        "    if args.local_rank == 0:\n",
        "        torch.distributed.barrier()\n",
        "\n",
        "    model.to(args.device)\n",
        "\n",
        "    train_sampler = RandomSampler if args.local_rank == -1 else DistributedSampler\n",
        "\n",
        "    labeled_trainloader = DataLoader(\n",
        "        labeled_dataset,\n",
        "        sampler=train_sampler(labeled_dataset),\n",
        "        batch_size=args.batch_size,\n",
        "        num_workers=args.num_workers,\n",
        "        drop_last=True)\n",
        "\n",
        "    unlabeled_trainloader = DataLoader(\n",
        "        unlabeled_dataset,\n",
        "        sampler=train_sampler(unlabeled_dataset),\n",
        "        batch_size=args.batch_size*args.mu,\n",
        "        num_workers=args.num_workers,\n",
        "        drop_last=True)\n",
        "\n",
        "    test_loader = DataLoader(\n",
        "        test_dataset,\n",
        "        sampler=SequentialSampler(test_dataset),\n",
        "        batch_size=args.batch_size,\n",
        "        num_workers=args.num_workers)\n",
        "\n",
        "    optimizer = optim.SGD(model.parameters(), lr=args.lr,\n",
        "                          momentum=0.9, nesterov=args.nesterov)\n",
        "\n",
        "    args.iteration = args.k_img // args.batch_size // args.world_size\n",
        "    args.total_steps = args.epochs * args.iteration\n",
        "    scheduler = get_cosine_schedule_with_warmup(\n",
        "        optimizer, args.warmup * args.iteration, args.total_steps)\n",
        "\n",
        "    if args.use_ema:\n",
        "        ema_model = ModelEMA(args, model, args.ema_decay, device)\n",
        "\n",
        "    start_epoch = 0\n",
        "\n",
        "    if args.resume:\n",
        "        logger.info(\"==> Resuming from checkpoint..\")\n",
        "        assert os.path.isfile(\n",
        "            args.resume), \"Error: no checkpoint directory found!\"\n",
        "        args.out = os.path.dirname(args.resume)\n",
        "        checkpoint = torch.load(args.resume)\n",
        "        best_acc = checkpoint['best_acc']\n",
        "        start_epoch = checkpoint['epoch']\n",
        "        model.load_state_dict(checkpoint['state_dict'])\n",
        "        if args.use_ema:\n",
        "            ema_model.ema.load_state_dict(checkpoint['ema_state_dict'])\n",
        "        optimizer.load_state_dict(checkpoint['optimizer'])\n",
        "        scheduler.load_state_dict(checkpoint['scheduler'])\n",
        "\n",
        "    if args.amp:\n",
        "        from apex import amp\n",
        "        model, optimizer = amp.initialize(\n",
        "            model, optimizer, opt_level=args.opt_level)\n",
        "\n",
        "    if args.local_rank != -1:\n",
        "        model = torch.nn.parallel.DistributedDataParallel(\n",
        "            model, device_ids=[args.local_rank],\n",
        "            output_device=args.local_rank, find_unused_parameters=True)\n",
        "\n",
        "    logger.info(\"***** Running training *****\")\n",
        "    logger.info(f\"  Task = {args.dataset}@{args.num_labeled}\")\n",
        "    logger.info(f\"  Num Epochs = {args.epochs}\")\n",
        "    logger.info(f\"  Batch size per GPU = {args.batch_size}\")\n",
        "    logger.info(\n",
        "        f\"  Total train batch size = {args.batch_size*args.world_size}\")\n",
        "    logger.info(f\"  Total optimization steps = {args.total_steps}\")\n",
        "\n",
        "    test_accs = []\n",
        "    model.zero_grad()\n",
        "    for epoch in range(start_epoch, args.epochs):\n",
        "\n",
        "        train_loss, train_loss_x, train_loss_u, mask_prob = train(\n",
        "            args, labeled_trainloader, unlabeled_trainloader,\n",
        "            model, optimizer, ema_model, scheduler, epoch)\n",
        "\n",
        "        if args.no_progress:\n",
        "            logger.info(\"Epoch {}. train_loss: {:.4f}. train_loss_x: {:.4f}. train_loss_u: {:.4f}.\"\n",
        "                        .format(epoch+1, train_loss, train_loss_x, train_loss_u))\n",
        "\n",
        "        if args.use_ema:\n",
        "            test_model = ema_model.ema\n",
        "        else:\n",
        "            test_model = model\n",
        "\n",
        "        test_loss, test_acc = test(args, test_loader, test_model, epoch)\n",
        "\n",
        "        if args.local_rank in [-1, 0]:\n",
        "            writer.add_scalar('train/1.train_loss', train_loss, epoch)\n",
        "            writer.add_scalar('train/2.train_loss_x', train_loss_x, epoch)\n",
        "            writer.add_scalar('train/3.train_loss_u', train_loss_u, epoch)\n",
        "            writer.add_scalar('train/4.mask', mask_prob, epoch)\n",
        "            writer.add_scalar('test/1.test_acc', test_acc, epoch)\n",
        "            writer.add_scalar('test/2.test_loss', test_loss, epoch)\n",
        "\n",
        "        is_best = test_acc > best_acc\n",
        "        best_acc = max(test_acc, best_acc)\n",
        "        if args.local_rank in [-1, 0]:\n",
        "            model_to_save = model.module if hasattr(model, \"module\") else model\n",
        "            if args.use_ema:\n",
        "                ema_to_save = ema_model.ema.module if hasattr(\n",
        "                    ema_model.ema, \"module\") else ema_model.ema\n",
        "            save_checkpoint({\n",
        "                'epoch': epoch + 1,\n",
        "                'state_dict': model_to_save.state_dict(),\n",
        "                'ema_state_dict': ema_to_save.state_dict() if args.use_ema else None,\n",
        "                'acc': test_acc,\n",
        "                'best_acc': best_acc,\n",
        "                'optimizer': optimizer.state_dict(),\n",
        "                'scheduler': scheduler.state_dict(),\n",
        "            }, is_best, args.out)\n",
        "\n",
        "        test_accs.append(test_acc)\n",
        "        logger.info('Best top-1 acc: {:.2f}'.format(best_acc))\n",
        "        logger.info('Mean top-1 acc: {:.2f}\\n'.format(\n",
        "            np.mean(test_accs[-20:])))\n",
        "\n",
        "    if args.local_rank in [-1, 0]:\n",
        "        writer.close()\n",
        "\n",
        "\n",
        "def train(args, labeled_trainloader, unlabeled_trainloader,\n",
        "          model, optimizer, ema_model, scheduler, epoch):\n",
        "    if args.amp:\n",
        "        from apex import amp\n",
        "    batch_time = AverageMeter()\n",
        "    data_time = AverageMeter()\n",
        "    losses = AverageMeter()\n",
        "    losses_x = AverageMeter()\n",
        "    losses_u = AverageMeter()\n",
        "    end = time.time()\n",
        "\n",
        "    if not args.no_progress:\n",
        "        p_bar = tqdm(range(args.iteration),\n",
        "                     disable=args.local_rank not in [-1, 0])\n",
        "\n",
        "    train_loader = zip(labeled_trainloader, unlabeled_trainloader)\n",
        "    model.train()\n",
        "    for batch_idx, (data_x, data_u) in enumerate(train_loader):\n",
        "        inputs_x, targets_x = data_x\n",
        "        (inputs_u_w, inputs_u_s), _ = data_u\n",
        "        data_time.update(time.time() - end)\n",
        "        batch_size = inputs_x.shape[0]\n",
        "        inputs = torch.cat((inputs_x, inputs_u_w, inputs_u_s)).to(args.device)\n",
        "        targets_x = targets_x.to(args.device)\n",
        "        logits = model(inputs)\n",
        "        logits_x = logits[:batch_size]\n",
        "        logits_u_w, logits_u_s = logits[batch_size:].chunk(2)\n",
        "        del logits\n",
        "\n",
        "        Lx = F.cross_entropy(logits_x, targets_x, reduction='mean')\n",
        "\n",
        "        pseudo_label = torch.softmax(logits_u_w.detach_(), dim=-1)\n",
        "        max_probs, targets_u = torch.max(pseudo_label, dim=-1)\n",
        "        mask = max_probs.ge(args.threshold).float()\n",
        "\n",
        "        Lu = (F.cross_entropy(logits_u_s, targets_u,\n",
        "                              reduction='none') * mask).mean()\n",
        "\n",
        "        loss = Lx + args.lambda_u * Lu\n",
        "\n",
        "        if args.amp:\n",
        "            with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
        "                scaled_loss.backward()\n",
        "        else:\n",
        "            loss.backward()\n",
        "\n",
        "        losses.update(loss.item())\n",
        "        losses_x.update(Lx.item())\n",
        "        losses_u.update(Lu.item())\n",
        "\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "        if args.use_ema:\n",
        "            ema_model.update(model)\n",
        "        model.zero_grad()\n",
        "\n",
        "        batch_time.update(time.time() - end)\n",
        "        end = time.time()\n",
        "        mask_prob = mask.mean().item()\n",
        "        if not args.no_progress:\n",
        "            p_bar.set_description(\"Train Epoch: {epoch}/{epochs:4}. Iter: {batch:4}/{iter:4}. LR: {lr:.6f}. Data: {data:.3f}s. Batch: {bt:.3f}s. Loss: {loss:.4f}. Loss_x: {loss_x:.4f}. Loss_u: {loss_u:.4f}. Mask: {mask:.4f}. \".format(\n",
        "                epoch=epoch + 1,\n",
        "                epochs=args.epochs,\n",
        "                batch=batch_idx + 1,\n",
        "                iter=args.iteration,\n",
        "                lr=scheduler.get_last_lr()[0],\n",
        "                data=data_time.avg,\n",
        "                bt=batch_time.avg,\n",
        "                loss=losses.avg,\n",
        "                loss_x=losses_x.avg,\n",
        "                loss_u=losses_u.avg,\n",
        "                mask=mask_prob))\n",
        "            p_bar.update()\n",
        "    if not args.no_progress:\n",
        "        p_bar.close()\n",
        "    return losses.avg, losses_x.avg, losses_u.avg, mask_prob\n",
        "\n",
        "\n",
        "def test(args, test_loader, model, epoch):\n",
        "    batch_time = AverageMeter()\n",
        "    data_time = AverageMeter()\n",
        "    losses = AverageMeter()\n",
        "    top1 = AverageMeter()\n",
        "    top5 = AverageMeter()\n",
        "    end = time.time()\n",
        "\n",
        "    if not args.no_progress:\n",
        "        test_loader = tqdm(test_loader,\n",
        "                           disable=args.local_rank not in [-1, 0])\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (inputs, targets) in enumerate(test_loader):\n",
        "            data_time.update(time.time() - end)\n",
        "            model.eval()\n",
        "\n",
        "            inputs = inputs.to(args.device)\n",
        "            targets = targets.to(args.device)\n",
        "            outputs = model(inputs)\n",
        "            loss = F.cross_entropy(outputs, targets)\n",
        "\n",
        "            prec1, prec5 = accuracy(outputs, targets, topk=(1, 5))\n",
        "            losses.update(loss.item(), inputs.shape[0])\n",
        "            top1.update(prec1.item(), inputs.shape[0])\n",
        "            top5.update(prec5.item(), inputs.shape[0])\n",
        "            batch_time.update(time.time() - end)\n",
        "            end = time.time()\n",
        "            if not args.no_progress:\n",
        "                test_loader.set_description(\"Test Iter: {batch:4}/{iter:4}. Data: {data:.3f}s. Batch: {bt:.3f}s. Loss: {loss:.4f}. top1: {top1:.2f}. top5: {top5:.2f}. \".format(\n",
        "                    batch=batch_idx + 1,\n",
        "                    iter=len(test_loader),\n",
        "                    data=data_time.avg,\n",
        "                    bt=batch_time.avg,\n",
        "                    loss=losses.avg,\n",
        "                    top1=top1.avg,\n",
        "                    top5=top5.avg,\n",
        "                ))\n",
        "        if not args.no_progress:\n",
        "            test_loader.close()\n",
        "\n",
        "    logger.info(\"top-1 acc: {:.2f}\".format(top1.avg))\n",
        "    logger.info(\"top-5 acc: {:.2f}\".format(top5.avg))\n",
        "    return losses.avg, top1.avg\n",
        "\n",
        "\n",
        "class ModelEMA(object):\n",
        "    def __init__(self, args, model, decay, device='', resume=''):\n",
        "        self.ema = deepcopy(model)\n",
        "        self.ema.eval()\n",
        "        self.decay = decay\n",
        "        self.device = device\n",
        "        self.wd = args.lr * args.wdecay\n",
        "        if device:\n",
        "            self.ema.to(device=device)\n",
        "        self.ema_has_module = hasattr(self.ema, 'module')\n",
        "        if resume:\n",
        "            self._load_checkpoint(resume)\n",
        "        for p in self.ema.parameters():\n",
        "            p.requires_grad_(False)\n",
        "\n",
        "    def _load_checkpoint(self, checkpoint_path):\n",
        "        checkpoint = torch.load(checkpoint_path)\n",
        "        assert isinstance(checkpoint, dict)\n",
        "        if 'ema_state_dict' in checkpoint:\n",
        "            new_state_dict = OrderedDict()\n",
        "            for k, v in checkpoint['ema_state_dict'].items():\n",
        "                if self.ema_has_module:\n",
        "                    name = 'module.' + k if not k.startswith('module') else k\n",
        "                else:\n",
        "                    name = k\n",
        "                new_state_dict[name] = v\n",
        "            self.ema.load_state_dict(new_state_dict)\n",
        "\n",
        "    def update(self, model):\n",
        "        needs_module = hasattr(model, 'module') and not self.ema_has_module\n",
        "        with torch.no_grad():\n",
        "            msd = model.state_dict()\n",
        "            for k, ema_v in self.ema.state_dict().items():\n",
        "                if needs_module:\n",
        "                    k = 'module.' + k\n",
        "                model_v = msd[k].detach()\n",
        "                if self.device:\n",
        "                    model_v = model_v.to(device=self.device)\n",
        "                ema_v.copy_(ema_v * self.decay + (1. - self.decay) * model_v)\n",
        "                # weight decay\n",
        "                if 'bn' not in k:\n",
        "                    msd[k] = msd[k] * (1. - self.wd)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    cudnn.benchmark = True\n",
        "    main()"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-15246bcd8a1a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtqdm\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcifar\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_cifar10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_cifar100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAverageMeter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'dataset'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EqnmbV2HnNrH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 831
        },
        "outputId": "a13ba7a9-aa34-42a6-a9cd-5e7ac2e60887"
      },
      "source": [
        "!python train.py --dataset cifar10 --num-labeled 4000 --epochs 100  --arch wideresnet --batch-size 64 --lr 0.03 --out cifar10@4000"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "07/21/2020 00:58:04 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n",
            "07/21/2020 00:58:04 - INFO - __main__ -   {'amp': False, 'arch': 'wideresnet', 'batch_size': 64, 'dataset': 'cifar10', 'device': device(type='cuda', index=0), 'ema_decay': 0.999, 'epochs': 100, 'gpu_id': 0, 'k_img': 65536, 'lambda_u': 1, 'local_rank': -1, 'lr': 0.03, 'model_depth': 28, 'model_width': 2, 'mu': 7, 'n_gpu': 1, 'nesterov': True, 'no_progress': False, 'num_classes': 10, 'num_labeled': 4000, 'num_workers': 4, 'opt_level': 'O1', 'out': 'cifar10@4000', 'resume': '', 'seed': -1, 'start_epoch': 0, 'threshold': 0.95, 'use_ema': True, 'warmup': 0, 'wdecay': 0.0005, 'world_size': 1}\n",
            "2020-07-21 00:58:04.325474: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
            "Files already downloaded and verified\n",
            "tcmalloc: large alloc 1409286144 bytes == 0x2ffe2000 @  0x7fd17e26c1e7 0x7fd17afe35e1 0x7fd17b047c78 0x7fd17b047d93 0x7fd17b0d2ed6 0x7fd17b0d3338 0x50c29e 0x507d64 0x509042 0x594931 0x549e5f 0x5513d1 0x5a9cbc 0x50a5c3 0x50cd96 0x509758 0x50a48d 0x50bfb4 0x509758 0x50a48d 0x50bfb4 0x507d64 0x50ae13 0x634c82 0x634d37 0x6384ef 0x639091 0x4b0d00 0x7fd17de69b97 0x5b250a\n",
            "07/21/2020 00:58:08 - INFO - dataset.cifar -   Dataset: CIFAR10\n",
            "07/21/2020 00:58:08 - INFO - dataset.cifar -   Labeled examples: 65536 Unlabeled examples: 458752\n",
            "07/21/2020 00:58:08 - INFO - models.wideresnet -   Model: WideResNet 28x2\n",
            "07/21/2020 00:58:08 - INFO - __main__ -   Total params: 1.47M\n",
            "07/21/2020 00:58:11 - INFO - __main__ -   ***** Running training *****\n",
            "07/21/2020 00:58:11 - INFO - __main__ -     Task = cifar10@4000\n",
            "07/21/2020 00:58:11 - INFO - __main__ -     Num Epochs = 100\n",
            "07/21/2020 00:58:11 - INFO - __main__ -     Batch size per GPU = 64\n",
            "07/21/2020 00:58:11 - INFO - __main__ -     Total train batch size = 64\n",
            "07/21/2020 00:58:11 - INFO - __main__ -     Total optimization steps = 102400\n",
            "Train Epoch: 1/ 100. Iter: 1024/1024. LR: 0.029997. Data: 0.049s. Batch: 0.377s. Loss: 1.2663. Loss_x: 1.2285. Loss_u: 0.0378. Mask: 0.1786. : 100% 1024/1024 [06:25<00:00,  2.65it/s]\n",
            "Test Iter:  157/ 157. Data: 0.004s. Batch: 0.018s. Loss: 5.1477. top1: 15.69. top5: 64.41. : 100% 157/157 [00:02<00:00, 53.73it/s]\n",
            "07/21/2020 01:04:40 - INFO - __main__ -   top-1 acc: 15.69\n",
            "07/21/2020 01:04:40 - INFO - __main__ -   top-5 acc: 64.41\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "07/21/2020 01:04:40 - INFO - __main__ -   Best top-1 acc: 15.69\n",
            "07/21/2020 01:04:40 - INFO - __main__ -   Mean top-1 acc: 15.69\n",
            "\n",
            "Train Epoch: 2/ 100. Iter: 1024/1024. LR: 0.029989. Data: 0.060s. Batch: 0.389s. Loss: 0.8078. Loss_x: 0.6627. Loss_u: 0.1451. Mask: 0.3772. : 100% 1024/1024 [06:38<00:00,  2.57it/s]\n",
            "Test Iter:  157/ 157. Data: 0.004s. Batch: 0.018s. Loss: 2.6575. top1: 44.10. top5: 87.15. : 100% 157/157 [00:02<00:00, 55.21it/s]\n",
            "07/21/2020 01:11:21 - INFO - __main__ -   top-1 acc: 44.10\n",
            "07/21/2020 01:11:21 - INFO - __main__ -   top-5 acc: 87.15\n",
            "07/21/2020 01:11:21 - INFO - __main__ -   Best top-1 acc: 44.10\n",
            "07/21/2020 01:11:21 - INFO - __main__ -   Mean top-1 acc: 29.89\n",
            "\n",
            "Train Epoch: 3/ 100. Iter: 1024/1024. LR: 0.029975. Data: 0.067s. Batch: 0.400s. Loss: 0.6121. Loss_x: 0.3633. Loss_u: 0.2487. Mask: 0.5491. : 100% 1024/1024 [06:49<00:00,  2.50it/s]\n",
            "Test Iter:  157/ 157. Data: 0.004s. Batch: 0.018s. Loss: 1.1864. top1: 68.59. top5: 95.76. : 100% 157/157 [00:02<00:00, 53.97it/s]\n",
            "07/21/2020 01:18:14 - INFO - __main__ -   top-1 acc: 68.59\n",
            "07/21/2020 01:18:14 - INFO - __main__ -   top-5 acc: 95.76\n",
            "07/21/2020 01:18:14 - INFO - __main__ -   Best top-1 acc: 68.59\n",
            "07/21/2020 01:18:14 - INFO - __main__ -   Mean top-1 acc: 42.79\n",
            "\n",
            "Train Epoch: 4/ 100. Iter: 1024/1024. LR: 0.029955. Data: 0.077s. Batch: 0.410s. Loss: 0.5001. Loss_x: 0.1874. Loss_u: 0.3127. Mask: 0.6317. : 100% 1024/1024 [07:00<00:00,  2.44it/s]\n",
            "Test Iter:  157/ 157. Data: 0.004s. Batch: 0.019s. Loss: 0.9250. top1: 75.32. top5: 97.90. : 100% 157/157 [00:03<00:00, 50.53it/s]\n",
            "07/21/2020 01:25:17 - INFO - __main__ -   top-1 acc: 75.32\n",
            "07/21/2020 01:25:17 - INFO - __main__ -   top-5 acc: 97.90\n",
            "07/21/2020 01:25:17 - INFO - __main__ -   Best top-1 acc: 75.32\n",
            "07/21/2020 01:25:17 - INFO - __main__ -   Mean top-1 acc: 50.92\n",
            "\n",
            "Train Epoch: 5/ 100. Iter:  703/1024. LR: 0.029938. Data: 0.077s. Batch: 0.413s. Loss: 0.4431. Loss_x: 0.1033. Loss_u: 0.3399. Mask: 0.6830. :  69% 703/1024 [04:50<02:15,  2.37it/s]"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}