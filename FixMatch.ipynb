{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FixMatch.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOyD3jFY4TD7auIrn20Cc13",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ayaanzhaque/APCSP-Create/blob/master/FixMatch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f1kdVROelfkV",
        "colab_type": "text"
      },
      "source": [
        "cifar.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "02OcSx8ok89d",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 368
        },
        "outputId": "0ae003d3-f6dd-4a1a-e4b9-7e813c4c79b1"
      },
      "source": [
        "import logging\n",
        "\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from torchvision import datasets\n",
        "from torchvision import transforms\n",
        "\n",
        "from .randaugment import RandAugmentMC\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "cifar10_mean = (0.4914, 0.4822, 0.4465)\n",
        "cifar10_std = (0.2471, 0.2435, 0.2616)\n",
        "cifar100_mean = (0.5071, 0.4867, 0.4408)\n",
        "cifar100_std = (0.2675, 0.2565, 0.2761)\n",
        "normal_mean = (0.5, 0.5, 0.5)\n",
        "normal_std = (0.5, 0.5, 0.5)\n",
        "\n",
        "\n",
        "def get_cifar10(root, num_labeled, num_expand_x, num_expand_u):\n",
        "    transform_labeled = transforms.Compose([\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.RandomCrop(size=32,\n",
        "                              padding=int(32*0.125),\n",
        "                              padding_mode='reflect'),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=cifar10_mean, std=cifar10_std)\n",
        "    ])\n",
        "    transform_val = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=cifar10_mean, std=cifar10_std)\n",
        "    ])\n",
        "    base_dataset = datasets.CIFAR10(root, train=True, download=True)\n",
        "\n",
        "    train_labeled_idxs, train_unlabeled_idxs = x_u_split(\n",
        "        base_dataset.targets, num_labeled, num_expand_x, num_expand_u, num_classes=10)\n",
        "\n",
        "    train_labeled_dataset = CIFAR10SSL(\n",
        "        root, train_labeled_idxs, train=True,\n",
        "        transform=transform_labeled)\n",
        "\n",
        "    train_unlabeled_dataset = CIFAR10SSL(\n",
        "        root, train_unlabeled_idxs, train=True,\n",
        "        transform=TransformFix(mean=cifar10_mean, std=cifar10_std))\n",
        "\n",
        "    test_dataset = datasets.CIFAR10(\n",
        "        root, train=False, transform=transform_val, download=False)\n",
        "    logger.info(\"Dataset: CIFAR10\")\n",
        "    logger.info(f\"Labeled examples: {len(train_labeled_idxs)}\"\n",
        "                f\" Unlabeled examples: {len(train_unlabeled_idxs)}\")\n",
        "\n",
        "    return train_labeled_dataset, train_unlabeled_dataset, test_dataset\n",
        "\n",
        "\n",
        "def get_cifar100(root, num_labeled, num_expand_x, num_expand_u):\n",
        "\n",
        "    transform_labeled = transforms.Compose([\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.RandomCrop(size=32,\n",
        "                              padding=int(32*0.125),\n",
        "                              padding_mode='reflect'),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=cifar100_mean, std=cifar100_std)])\n",
        "\n",
        "    transform_val = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=cifar100_mean, std=cifar100_std)])\n",
        "\n",
        "    base_dataset = datasets.CIFAR100(\n",
        "        root, train=True, download=True)\n",
        "\n",
        "    train_labeled_idxs, train_unlabeled_idxs =x_u_split(\n",
        "        base_dataset.targets, num_labeled, num_expand_x, num_expand_u, num_classes=100)\n",
        "\n",
        "    train_labeled_dataset = CIFAR100SSL(\n",
        "        root, train_labeled_idxs, train=True,\n",
        "        transform=transform_labeled)\n",
        "\n",
        "    train_unlabeled_dataset = CIFAR100SSL(\n",
        "        root, train_unlabeled_idxs, train=True,\n",
        "        transform=TransformFix(mean=cifar100_mean, std=cifar100_std))\n",
        "\n",
        "    test_dataset = datasets.CIFAR100(\n",
        "        root, train=False, transform=transform_val, download=False)\n",
        "\n",
        "    logger.info(\"Dataset: CIFAR100\")\n",
        "    logger.info(f\"Labeled examples: {len(train_labeled_idxs)}\"\n",
        "                f\" Unlabeled examples: {len(train_unlabeled_idxs)}\")\n",
        "\n",
        "    return train_labeled_dataset, train_unlabeled_dataset, test_dataset\n",
        "\n",
        "\n",
        "def x_u_split(labels,\n",
        "              num_labeled,\n",
        "              num_expand_x,\n",
        "              num_expand_u,\n",
        "              num_classes):\n",
        "    label_per_class = num_labeled // num_classes\n",
        "    labels = np.array(labels)\n",
        "    labeled_idx = []\n",
        "    unlabeled_idx = []\n",
        "    for i in range(num_classes):\n",
        "        idx = np.where(labels == i)[0]\n",
        "        np.random.shuffle(idx)\n",
        "        labeled_idx.extend(idx[:label_per_class])\n",
        "        unlabeled_idx.extend(idx[label_per_class:])\n",
        "\n",
        "    exapand_labeled = num_expand_x // len(labeled_idx)\n",
        "    exapand_unlabeled = num_expand_u // len(unlabeled_idx)\n",
        "    labeled_idx = np.hstack(\n",
        "        [labeled_idx for _ in range(exapand_labeled)])\n",
        "    unlabeled_idx = np.hstack(\n",
        "        [unlabeled_idx for _ in range(exapand_unlabeled)])\n",
        "\n",
        "    if len(labeled_idx) < num_expand_x:\n",
        "        diff = num_expand_x - len(labeled_idx)\n",
        "        labeled_idx = np.hstack(\n",
        "            (labeled_idx, np.random.choice(labeled_idx, diff)))\n",
        "    else:\n",
        "        assert len(labeled_idx) == num_expand_x\n",
        "\n",
        "    if len(unlabeled_idx) < num_expand_u:\n",
        "        diff = num_expand_u - len(unlabeled_idx)\n",
        "        unlabeled_idx = np.hstack(\n",
        "            (unlabeled_idx, np.random.choice(unlabeled_idx, diff)))\n",
        "    else:\n",
        "        assert len(unlabeled_idx) == num_expand_u\n",
        "\n",
        "    return labeled_idx, unlabeled_idx\n",
        "\n",
        "\n",
        "class TransformFix(object):\n",
        "    def __init__(self, mean, std):\n",
        "        self.weak = transforms.Compose([\n",
        "            transforms.RandomHorizontalFlip(),\n",
        "            transforms.RandomCrop(size=32,\n",
        "                                  padding=int(32*0.125),\n",
        "                                  padding_mode='reflect')])\n",
        "        self.strong = transforms.Compose([\n",
        "            transforms.RandomHorizontalFlip(),\n",
        "            transforms.RandomCrop(size=32,\n",
        "                                  padding=int(32*0.125),\n",
        "                                  padding_mode='reflect'),\n",
        "            RandAugmentMC(n=2, m=10)])\n",
        "        self.normalize = transforms.Compose([\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=mean, std=std)])\n",
        "\n",
        "    def __call__(self, x):\n",
        "        weak = self.weak(x)\n",
        "        strong = self.strong(x)\n",
        "        return self.normalize(weak), self.normalize(strong)\n",
        "\n",
        "\n",
        "class CIFAR10SSL(datasets.CIFAR10):\n",
        "    def __init__(self, root, indexs, train=True,\n",
        "                 transform=None, target_transform=None,\n",
        "                 download=False):\n",
        "        super().__init__(root, train=train,\n",
        "                         transform=transform,\n",
        "                         target_transform=target_transform,\n",
        "                         download=download)\n",
        "        if indexs is not None:\n",
        "            self.data = self.data[indexs]\n",
        "            self.targets = np.array(self.targets)[indexs]\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        img, target = self.data[index], self.targets[index]\n",
        "        img = Image.fromarray(img)\n",
        "\n",
        "        if self.transform is not None:\n",
        "            img = self.transform(img)\n",
        "\n",
        "        if self.target_transform is not None:\n",
        "            target = self.target_transform(target)\n",
        "\n",
        "        return img, target\n",
        "\n",
        "\n",
        "class CIFAR100SSL(datasets.CIFAR100):\n",
        "    def __init__(self, root, indexs, train=True,\n",
        "                 transform=None, target_transform=None,\n",
        "                 download=False):\n",
        "        super().__init__(root, train=train,\n",
        "                         transform=transform,\n",
        "                         target_transform=target_transform,\n",
        "                         download=download)\n",
        "        if indexs is not None:\n",
        "            self.data = self.data[indexs]\n",
        "            self.targets = np.array(self.targets)[indexs]\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        img, target = self.data[index], self.targets[index]\n",
        "        img = Image.fromarray(img)\n",
        "\n",
        "        if self.transform is not None:\n",
        "            img = self.transform(img)\n",
        "\n",
        "        if self.target_transform is not None:\n",
        "            target = self.target_transform(target)\n",
        "\n",
        "        return img, target\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-33dbdfd61bb4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchvision\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mrandaugment\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRandAugmentMC\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mlogger\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetLogger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named '__main__.randaugment'; '__main__' is not a package",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OYJoSO6HljkJ",
        "colab_type": "text"
      },
      "source": [
        "randaugment.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aeb4MHixldQM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import logging\n",
        "import random\n",
        "\n",
        "import numpy as np\n",
        "import PIL\n",
        "import PIL.ImageOps\n",
        "import PIL.ImageEnhance\n",
        "import PIL.ImageDraw\n",
        "from PIL import Image\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "PARAMETER_MAX = 10\n",
        "\n",
        "\n",
        "def AutoContrast(img, **kwarg):\n",
        "    return PIL.ImageOps.autocontrast(img)\n",
        "\n",
        "\n",
        "def Brightness(img, v, max_v, bias=0):\n",
        "    v = _float_parameter(v, max_v) + bias\n",
        "    return PIL.ImageEnhance.Brightness(img).enhance(v)\n",
        "\n",
        "\n",
        "def Color(img, v, max_v, bias=0):\n",
        "    v = _float_parameter(v, max_v) + bias\n",
        "    return PIL.ImageEnhance.Color(img).enhance(v)\n",
        "\n",
        "\n",
        "def Contrast(img, v, max_v, bias=0):\n",
        "    v = _float_parameter(v, max_v) + bias\n",
        "    return PIL.ImageEnhance.Contrast(img).enhance(v)\n",
        "\n",
        "\n",
        "def Cutout(img, v, max_v, bias=0):\n",
        "    if v == 0:\n",
        "        return img\n",
        "    v = _float_parameter(v, max_v) + bias\n",
        "    v = int(v * min(img.size))\n",
        "    return CutoutAbs(img, v)\n",
        "\n",
        "\n",
        "def CutoutAbs(img, v, **kwarg):\n",
        "    w, h = img.size\n",
        "    x0 = np.random.uniform(0, w)\n",
        "    y0 = np.random.uniform(0, h)\n",
        "    x0 = int(max(0, x0 - v / 2.))\n",
        "    y0 = int(max(0, y0 - v / 2.))\n",
        "    x1 = int(min(w, x0 + v))\n",
        "    y1 = int(min(h, y0 + v))\n",
        "    xy = (x0, y0, x1, y1)\n",
        "    # gray\n",
        "    color = (127, 127, 127)\n",
        "    img = img.copy()\n",
        "    PIL.ImageDraw.Draw(img).rectangle(xy, color)\n",
        "    return img\n",
        "\n",
        "\n",
        "def Equalize(img, **kwarg):\n",
        "    return PIL.ImageOps.equalize(img)\n",
        "\n",
        "\n",
        "def Identity(img, **kwarg):\n",
        "    return img\n",
        "\n",
        "\n",
        "def Invert(img, **kwarg):\n",
        "    return PIL.ImageOps.invert(img)\n",
        "\n",
        "\n",
        "def Posterize(img, v, max_v, bias=0):\n",
        "    v = _int_parameter(v, max_v) + bias\n",
        "    return PIL.ImageOps.posterize(img, v)\n",
        "\n",
        "\n",
        "def Rotate(img, v, max_v, bias=0):\n",
        "    v = _int_parameter(v, max_v) + bias\n",
        "    if random.random() < 0.5:\n",
        "        v = -v\n",
        "    return img.rotate(v)\n",
        "\n",
        "\n",
        "def Sharpness(img, v, max_v, bias=0):\n",
        "    v = _float_parameter(v, max_v) + bias\n",
        "    return PIL.ImageEnhance.Sharpness(img).enhance(v)\n",
        "\n",
        "\n",
        "def ShearX(img, v, max_v, bias=0):\n",
        "    v = _float_parameter(v, max_v) + bias\n",
        "    if random.random() < 0.5:\n",
        "        v = -v\n",
        "    return img.transform(img.size, PIL.Image.AFFINE, (1, v, 0, 0, 1, 0))\n",
        "\n",
        "\n",
        "def ShearY(img, v, max_v, bias=0):\n",
        "    v = _float_parameter(v, max_v) + bias\n",
        "    if random.random() < 0.5:\n",
        "        v = -v\n",
        "    return img.transform(img.size, PIL.Image.AFFINE, (1, 0, 0, v, 1, 0))\n",
        "\n",
        "\n",
        "def Solarize(img, v, max_v, bias=0):\n",
        "    v = _int_parameter(v, max_v) + bias\n",
        "    return PIL.ImageOps.solarize(img, 256 - v)\n",
        "\n",
        "\n",
        "def SolarizeAdd(img, v, max_v, bias=0, threshold=128):\n",
        "    v = _int_parameter(v, max_v) + bias\n",
        "    if random.random() < 0.5:\n",
        "        v = -v\n",
        "    img_np = np.array(img).astype(np.int)\n",
        "    img_np = img_np + v\n",
        "    img_np = np.clip(img_np, 0, 255)\n",
        "    img_np = img_np.astype(np.uint8)\n",
        "    img = Image.fromarray(img_np)\n",
        "    return PIL.ImageOps.solarize(img, threshold)\n",
        "\n",
        "\n",
        "def TranslateX(img, v, max_v, bias=0):\n",
        "    v = _float_parameter(v, max_v) + bias\n",
        "    if random.random() < 0.5:\n",
        "        v = -v\n",
        "    v = int(v * img.size[0])\n",
        "    return img.transform(img.size, PIL.Image.AFFINE, (1, 0, v, 0, 1, 0))\n",
        "\n",
        "\n",
        "def TranslateY(img, v, max_v, bias=0):\n",
        "    v = _float_parameter(v, max_v) + bias\n",
        "    if random.random() < 0.5:\n",
        "        v = -v\n",
        "    v = int(v * img.size[1])\n",
        "    return img.transform(img.size, PIL.Image.AFFINE, (1, 0, 0, 0, 1, v))\n",
        "\n",
        "\n",
        "def _float_parameter(v, max_v):\n",
        "    return float(v) * max_v / PARAMETER_MAX\n",
        "\n",
        "\n",
        "def _int_parameter(v, max_v):\n",
        "    return int(v * max_v / PARAMETER_MAX)\n",
        "\n",
        "\n",
        "def fixmatch_augment_pool():\n",
        "    # FixMatch paper\n",
        "    augs = [(AutoContrast, None, None),\n",
        "            (Brightness, 0.9, 0.05),\n",
        "            (Color, 0.9, 0.05),\n",
        "            (Contrast, 0.9, 0.05),\n",
        "            (Equalize, None, None),\n",
        "            (Identity, None, None),\n",
        "            (Posterize, 4, 4),\n",
        "            (Rotate, 30, 0),\n",
        "            (Sharpness, 0.9, 0.05),\n",
        "            (ShearX, 0.3, 0),\n",
        "            (ShearY, 0.3, 0),\n",
        "            (Solarize, 256, 0),\n",
        "            (TranslateX, 0.3, 0),\n",
        "            (TranslateY, 0.3, 0)]\n",
        "    return augs\n",
        "\n",
        "\n",
        "def my_augment_pool():\n",
        "    # Test\n",
        "    augs = [(AutoContrast, None, None),\n",
        "            (Brightness, 1.8, 0.1),\n",
        "            (Color, 1.8, 0.1),\n",
        "            (Contrast, 1.8, 0.1),\n",
        "            (Cutout, 0.2, 0),\n",
        "            (Equalize, None, None),\n",
        "            (Invert, None, None),\n",
        "            (Posterize, 4, 4),\n",
        "            (Rotate, 30, 0),\n",
        "            (Sharpness, 1.8, 0.1),\n",
        "            (ShearX, 0.3, 0),\n",
        "            (ShearY, 0.3, 0),\n",
        "            (Solarize, 256, 0),\n",
        "            (SolarizeAdd, 110, 0),\n",
        "            (TranslateX, 0.45, 0),\n",
        "            (TranslateY, 0.45, 0)]\n",
        "    return augs\n",
        "\n",
        "\n",
        "class RandAugmentPC(object):\n",
        "    def __init__(self, n, m):\n",
        "        assert n >= 1\n",
        "        assert 1 <= m <= 10\n",
        "        self.n = n\n",
        "        self.m = m\n",
        "        self.augment_pool = my_augment_pool()\n",
        "\n",
        "    def __call__(self, img):\n",
        "        ops = random.choices(self.augment_pool, k=self.n)\n",
        "        for op, max_v, bias in ops:\n",
        "            prob = np.random.uniform(0.2, 0.8)\n",
        "            if random.random() + prob >= 1:\n",
        "                img = op(img, v=self.m, max_v=max_v, bias=bias)\n",
        "        img = CutoutAbs(img, 16)\n",
        "        return img\n",
        "\n",
        "\n",
        "class RandAugmentMC(object):\n",
        "    def __init__(self, n, m):\n",
        "        assert n >= 1\n",
        "        assert 1 <= m <= 10\n",
        "        self.n = n\n",
        "        self.m = m\n",
        "        self.augment_pool = fixmatch_augment_pool()\n",
        "\n",
        "    def __call__(self, img):\n",
        "        ops = random.choices(self.augment_pool, k=self.n)\n",
        "        for op, max_v, bias in ops:\n",
        "            v = np.random.randint(1, self.m)\n",
        "            if random.random() < 0.5:\n",
        "                img = op(img, v=v, max_v=max_v, bias=bias)\n",
        "        img = CutoutAbs(img, 16)\n",
        "        return img\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hVNXploflxM9",
        "colab_type": "text"
      },
      "source": [
        "resnext.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V1edrMIolp29",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import logging\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "\n",
        "def mish(x):\n",
        "    \"\"\"Mish: A Self Regularized Non-Monotonic Neural Activation Function (https://arxiv.org/abs/1908.08681)\"\"\"\n",
        "    return x * torch.tanh(F.softplus(x))\n",
        "\n",
        "\n",
        "class PSBatchNorm2d(nn.BatchNorm2d):\n",
        "    \"\"\"How Does BN Increase Collapsed Neural Network Filters? (https://arxiv.org/abs/2001.11216)\"\"\"\n",
        "\n",
        "    def __init__(self, num_features, alpha=0.1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True):\n",
        "        super().__init__(num_features, eps, momentum, affine, track_running_stats)\n",
        "        self.alpha = alpha\n",
        "\n",
        "    def forward(self, x):\n",
        "        return super().forward(x) + self.alpha\n",
        "\n",
        "\n",
        "class ResNeXtBottleneck(nn.Module):\n",
        "    \"\"\"\n",
        "    RexNeXt bottleneck type C (https://github.com/facebookresearch/ResNeXt/blob/master/models/resnext.lua)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, stride,\n",
        "                 cardinality, base_width, widen_factor):\n",
        "        \"\"\" Constructor\n",
        "        Args:\n",
        "            in_channels: input channel dimensionality\n",
        "            out_channels: output channel dimensionality\n",
        "            stride: conv stride. Replaces pooling layer.\n",
        "            cardinality: num of convolution groups.\n",
        "            base_width: base number of channels in each group.\n",
        "            widen_factor: factor to reduce the input dimensionality before convolution.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        width_ratio = out_channels / (widen_factor * 64.)\n",
        "        D = cardinality * int(base_width * width_ratio)\n",
        "        self.conv_reduce = nn.Conv2d(\n",
        "            in_channels, D, kernel_size=1, stride=1, padding=0, bias=False)\n",
        "        self.bn_reduce = PSBatchNorm2d(D, momentum=0.001)\n",
        "        self.conv_conv = nn.Conv2d(D, D,\n",
        "                                   kernel_size=3, stride=stride, padding=1,\n",
        "                                   groups=cardinality, bias=False)\n",
        "        self.bn = PSBatchNorm2d(D, momentum=0.001)\n",
        "        self.act = mish\n",
        "        self.conv_expand = nn.Conv2d(\n",
        "            D, out_channels, kernel_size=1, stride=1, padding=0, bias=False)\n",
        "        self.bn_expand = PSBatchNorm2d(out_channels, momentum=0.001)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if in_channels != out_channels:\n",
        "            self.shortcut.add_module('shortcut_conv',\n",
        "                                     nn.Conv2d(in_channels, out_channels,\n",
        "                                               kernel_size=1,\n",
        "                                               stride=stride,\n",
        "                                               padding=0,\n",
        "                                               bias=False))\n",
        "            self.shortcut.add_module(\n",
        "                'shortcut_bn', PSBatchNorm2d(out_channels, momentum=0.001))\n",
        "\n",
        "    def forward(self, x):\n",
        "        bottleneck = self.conv_reduce.forward(x)\n",
        "        bottleneck = self.act(self.bn_reduce.forward(bottleneck))\n",
        "        bottleneck = self.conv_conv.forward(bottleneck)\n",
        "        bottleneck = self.act(self.bn.forward(bottleneck))\n",
        "        bottleneck = self.conv_expand.forward(bottleneck)\n",
        "        bottleneck = self.bn_expand.forward(bottleneck)\n",
        "        residual = self.shortcut.forward(x)\n",
        "        return self.act(residual + bottleneck)\n",
        "\n",
        "\n",
        "class CifarResNeXt(nn.Module):\n",
        "    \"\"\"\n",
        "    ResNext optimized for the Cifar dataset, as specified in\n",
        "    https://arxiv.org/pdf/1611.05431.pdf\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, cardinality, depth, num_classes,\n",
        "                 base_width, widen_factor=4):\n",
        "        \"\"\" Constructor\n",
        "        Args:\n",
        "            cardinality: number of convolution groups.\n",
        "            depth: number of layers.\n",
        "            nlabels: number of classes\n",
        "            base_width: base number of channels in each group.\n",
        "            widen_factor: factor to adjust the channel dimensionality\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.cardinality = cardinality\n",
        "        self.depth = depth\n",
        "        self.block_depth = (self.depth - 2) // 9\n",
        "        self.base_width = base_width\n",
        "        self.widen_factor = widen_factor\n",
        "        self.nlabels = num_classes\n",
        "        self.output_size = 64\n",
        "        self.stages = [64, 64 * self.widen_factor, 128 *\n",
        "                       self.widen_factor, 256 * self.widen_factor]\n",
        "\n",
        "        self.conv_1_3x3 = nn.Conv2d(3, 64, 3, 1, 1, bias=False)\n",
        "        self.bn_1 = PSBatchNorm2d(64, momentum=0.001)\n",
        "        self.act = mish\n",
        "        self.stage_1 = self.block('stage_1', self.stages[0], self.stages[1], 1)\n",
        "        self.stage_2 = self.block('stage_2', self.stages[1], self.stages[2], 2)\n",
        "        self.stage_3 = self.block('stage_3', self.stages[2], self.stages[3], 2)\n",
        "        self.classifier = nn.Linear(self.stages[3], num_classes)\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight,\n",
        "                                        mode='fan_out',\n",
        "                                        nonlinearity='leaky_relu')\n",
        "            elif isinstance(m, PSBatchNorm2d):\n",
        "                nn.init.constant_(m.weight, 1.0)\n",
        "                nn.init.constant_(m.bias, 0.0)\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                nn.init.xavier_normal_(m.weight)\n",
        "                nn.init.constant_(m.bias, 0.0)\n",
        "\n",
        "    def block(self, name, in_channels, out_channels, pool_stride=2):\n",
        "        \"\"\" Stack n bottleneck modules where n is inferred from the depth of the network.\n",
        "        Args:\n",
        "            name: string name of the current block.\n",
        "            in_channels: number of input channels\n",
        "            out_channels: number of output channels\n",
        "            pool_stride: factor to reduce the spatial dimensionality in the first bottleneck of the block.\n",
        "        Returns: a Module consisting of n sequential bottlenecks.\n",
        "        \"\"\"\n",
        "        block = nn.Sequential()\n",
        "        for bottleneck in range(self.block_depth):\n",
        "            name_ = '%s_bottleneck_%d' % (name, bottleneck)\n",
        "            if bottleneck == 0:\n",
        "                block.add_module(name_, ResNeXtBottleneck(in_channels,\n",
        "                                                          out_channels,\n",
        "                                                          pool_stride,\n",
        "                                                          self.cardinality,\n",
        "                                                          self.base_width,\n",
        "                                                          self.widen_factor))\n",
        "            else:\n",
        "                block.add_module(name_,\n",
        "                                 ResNeXtBottleneck(out_channels,\n",
        "                                                   out_channels,\n",
        "                                                   1,\n",
        "                                                   self.cardinality,\n",
        "                                                   self.base_width,\n",
        "                                                   self.widen_factor))\n",
        "        return block\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv_1_3x3.forward(x)\n",
        "        x = self.act(self.bn_1.forward(x))\n",
        "        x = self.stage_1.forward(x)\n",
        "        x = self.stage_2.forward(x)\n",
        "        x = self.stage_3.forward(x)\n",
        "        x = F.adaptive_avg_pool2d(x, 1)\n",
        "        x = x.view(-1, self.stages[3])\n",
        "        return self.classifier(x)\n",
        "\n",
        "\n",
        "def build_resnext(cardinality, depth, width, num_classes):\n",
        "    logger.info(f\"Model: ResNeXt {depth+1}x{width}\")\n",
        "    return CifarResNeXt(cardinality=cardinality,\n",
        "                        depth=depth,\n",
        "                        base_width=width,\n",
        "                        num_classes=num_classes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XL9h_627l4xb",
        "colab_type": "text"
      },
      "source": [
        "wideresnet.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rp-j1gOBl1ZY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import logging\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "\n",
        "def mish(x):\n",
        "    \"\"\"Mish: A Self Regularized Non-Monotonic Neural Activation Function (https://arxiv.org/abs/1908.08681)\"\"\"\n",
        "    return x * torch.tanh(F.softplus(x))\n",
        "\n",
        "\n",
        "class PSBatchNorm2d(nn.BatchNorm2d):\n",
        "    \"\"\"How Does BN Increase Collapsed Neural Network Filters? (https://arxiv.org/abs/2001.11216)\"\"\"\n",
        "\n",
        "    def __init__(self, num_features, alpha=0.1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True):\n",
        "        super().__init__(num_features, eps, momentum, affine, track_running_stats)\n",
        "        self.alpha = alpha\n",
        "\n",
        "    def forward(self, x):\n",
        "        return super().forward(x) + self.alpha\n",
        "\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    def __init__(self, in_planes, out_planes, stride, drop_rate=0.0, activate_before_residual=False):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.bn1 = PSBatchNorm2d(in_planes, momentum=0.001)\n",
        "        self.relu1 = nn.LeakyReLU(negative_slope=0.1, inplace=True)\n",
        "        self.conv1 = nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
        "                               padding=1, bias=False)\n",
        "        self.bn2 = PSBatchNorm2d(out_planes, momentum=0.001)\n",
        "        self.relu2 = nn.LeakyReLU(negative_slope=0.1, inplace=True)\n",
        "        self.conv2 = nn.Conv2d(out_planes, out_planes, kernel_size=3, stride=1,\n",
        "                               padding=1, bias=False)\n",
        "        self.drop_rate = drop_rate\n",
        "        self.equalInOut = (in_planes == out_planes)\n",
        "        self.convShortcut = (not self.equalInOut) and nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride,\n",
        "                                                                padding=0, bias=False) or None\n",
        "        self.activate_before_residual = activate_before_residual\n",
        "\n",
        "    def forward(self, x):\n",
        "        if not self.equalInOut and self.activate_before_residual == True:\n",
        "            x = self.relu1(self.bn1(x))\n",
        "        else:\n",
        "            out = self.relu1(self.bn1(x))\n",
        "        out = self.relu2(self.bn2(self.conv1(out if self.equalInOut else x)))\n",
        "        if self.drop_rate > 0:\n",
        "            out = F.dropout(out, p=self.drop_rate, training=self.training)\n",
        "        out = self.conv2(out)\n",
        "        return torch.add(x if self.equalInOut else self.convShortcut(x), out)\n",
        "\n",
        "\n",
        "class NetworkBlock(nn.Module):\n",
        "    def __init__(self, nb_layers, in_planes, out_planes, block, stride, drop_rate=0.0, activate_before_residual=False):\n",
        "        super(NetworkBlock, self).__init__()\n",
        "        self.layer = self._make_layer(\n",
        "            block, in_planes, out_planes, nb_layers, stride, drop_rate, activate_before_residual)\n",
        "\n",
        "    def _make_layer(self, block, in_planes, out_planes, nb_layers, stride, drop_rate, activate_before_residual):\n",
        "        layers = []\n",
        "        for i in range(int(nb_layers)):\n",
        "            layers.append(block(i == 0 and in_planes or out_planes, out_planes,\n",
        "                                i == 0 and stride or 1, drop_rate, activate_before_residual))\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.layer(x)\n",
        "\n",
        "\n",
        "class WideResNet(nn.Module):\n",
        "    def __init__(self, num_classes, depth=28, widen_factor=2, drop_rate=0.0):\n",
        "        super(WideResNet, self).__init__()\n",
        "        channels = [16, 16*widen_factor, 32*widen_factor, 64*widen_factor]\n",
        "        assert((depth - 4) % 6 == 0)\n",
        "        n = (depth - 4) / 6\n",
        "        block = BasicBlock\n",
        "        # 1st conv before any network block\n",
        "        self.conv1 = nn.Conv2d(3, channels[0], kernel_size=3, stride=1,\n",
        "                               padding=1, bias=False)\n",
        "        # 1st block\n",
        "        self.block1 = NetworkBlock(\n",
        "            n, channels[0], channels[1], block, 1, drop_rate, activate_before_residual=True)\n",
        "        # 2nd block\n",
        "        self.block2 = NetworkBlock(\n",
        "            n, channels[1], channels[2], block, 2, drop_rate)\n",
        "        # 3rd block\n",
        "        self.block3 = NetworkBlock(\n",
        "            n, channels[2], channels[3], block, 2, drop_rate)\n",
        "        # global average pooling and classifier\n",
        "        self.bn1 = PSBatchNorm2d(channels[3], momentum=0.001)\n",
        "        self.relu = nn.LeakyReLU(negative_slope=0.1, inplace=True)\n",
        "        self.fc = nn.Linear(channels[3], num_classes)\n",
        "        self.channels = channels[3]\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight,\n",
        "                                        mode='fan_out',\n",
        "                                        nonlinearity='leaky_relu')\n",
        "            elif isinstance(m, PSBatchNorm2d):\n",
        "                nn.init.constant_(m.weight, 1.0)\n",
        "                nn.init.constant_(m.bias, 0.0)\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                nn.init.xavier_normal_(m.weight)\n",
        "                nn.init.constant_(m.bias, 0.0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.conv1(x)\n",
        "        out = self.block1(out)\n",
        "        out = self.block2(out)\n",
        "        out = self.block3(out)\n",
        "        out = self.relu(self.bn1(out))\n",
        "        out = F.adaptive_avg_pool2d(out, 1)\n",
        "        out = out.view(-1, self.channels)\n",
        "        return self.fc(out)\n",
        "\n",
        "\n",
        "def build_wideresnet(depth, widen_factor, dropout, num_classes):\n",
        "    logger.info(f\"Model: WideResNet {depth}x{widen_factor}\")\n",
        "    return WideResNet(depth=depth,\n",
        "                      widen_factor=widen_factor,\n",
        "                      drop_rate=dropout,\n",
        "                      num_classes=num_classes)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LWf38t4ImM8i",
        "colab_type": "text"
      },
      "source": [
        "train.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "98QxvO-4mNoU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import argparse\n",
        "import logging\n",
        "import math\n",
        "import os\n",
        "import random\n",
        "import shutil\n",
        "import time\n",
        "from copy import deepcopy\n",
        "from collections import OrderedDict\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.backends.cudnn as cudnn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import LambdaLR\n",
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
        "from torch.utils.data.distributed import DistributedSampler\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from tqdm import tqdm\n",
        "\n",
        "from dataset.cifar import get_cifar10, get_cifar100\n",
        "from utils import AverageMeter, accuracy\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "DATASET_GETTERS = {'cifar10': get_cifar10,\n",
        "                   'cifar100': get_cifar100}\n",
        "best_acc = 0\n",
        "\n",
        "\n",
        "def save_checkpoint(state, is_best, checkpoint, filename='checkpoint.pth.tar'):\n",
        "    filepath = os.path.join(checkpoint, filename)\n",
        "    torch.save(state, filepath)\n",
        "    if is_best:\n",
        "        shutil.copyfile(filepath, os.path.join(checkpoint,\n",
        "                                               'model_best.pth.tar'))\n",
        "\n",
        "\n",
        "def set_seed(args):\n",
        "    random.seed(args.seed)\n",
        "    np.random.seed(args.seed)\n",
        "    torch.manual_seed(args.seed)\n",
        "    torch.cuda.manual_seed(args.seed)\n",
        "    if args.n_gpu > 0:\n",
        "        torch.cuda.manual_seed_all(args.seed)\n",
        "\n",
        "\n",
        "def get_cosine_schedule_with_warmup(optimizer,\n",
        "                                    num_warmup_steps,\n",
        "                                    num_training_steps,\n",
        "                                    num_cycles=7./16.,\n",
        "                                    last_epoch=-1):\n",
        "    def _lr_lambda(current_step):\n",
        "        if current_step < num_warmup_steps:\n",
        "            return float(current_step) / float(max(1, num_warmup_steps))\n",
        "        no_progress = float(current_step - num_warmup_steps) / \\\n",
        "            float(max(1, num_training_steps - num_warmup_steps))\n",
        "        return max(0., math.cos(math.pi * num_cycles * no_progress))\n",
        "\n",
        "    return LambdaLR(optimizer, _lr_lambda, last_epoch)\n",
        "\n",
        "\n",
        "def main():\n",
        "    parser = argparse.ArgumentParser(description='PyTorch FixMatch Training')\n",
        "    parser.add_argument('--gpu-id', default='0', type=int,\n",
        "                        help='id(s) for CUDA_VISIBLE_DEVICES')\n",
        "    parser.add_argument('--num-workers', type=int, default=4,\n",
        "                        help='number of workers')\n",
        "    parser.add_argument('--dataset', default='cifar10', type=str,\n",
        "                        choices=['cifar10', 'cifar100'],\n",
        "                        help='dataset name')\n",
        "    parser.add_argument('--num-labeled', type=int, default=4000,\n",
        "                        help='number of labeled data')\n",
        "    parser.add_argument('--arch', default='wideresnet', type=str,\n",
        "                        choices=['wideresnet', 'resnext'],\n",
        "                        help='dataset name')\n",
        "    parser.add_argument('--epochs', default=100, type=int,\n",
        "                        help='number of total epochs to run')\n",
        "    parser.add_argument('--start-epoch', default=0, type=int,\n",
        "                        help='manual epoch number (useful on restarts)')\n",
        "    parser.add_argument('--batch-size', default=64, type=int,\n",
        "                        help='train batchsize')\n",
        "    parser.add_argument('--lr', '--learning-rate', default=0.03, type=float,\n",
        "                        help='initial learning rate')\n",
        "    parser.add_argument('--warmup', default=0, type=float,\n",
        "                        help='warmup epochs (unlabeled data based)')\n",
        "    parser.add_argument('--wdecay', default=5e-4, type=float,\n",
        "                        help='weight decay')\n",
        "    parser.add_argument('--nesterov', action='store_true', default=True,\n",
        "                        help='use nesterov momentum')\n",
        "    parser.add_argument('--use-ema', action='store_true', default=True,\n",
        "                        help='use EMA model')\n",
        "    parser.add_argument('--ema-decay', default=0.999, type=float,\n",
        "                        help='EMA decay rate')\n",
        "    parser.add_argument('--mu', default=7, type=int,\n",
        "                        help='coefficient of unlabeled batch size')\n",
        "    parser.add_argument('--lambda-u', default=1, type=float,\n",
        "                        help='coefficient of unlabeled loss')\n",
        "    parser.add_argument('--threshold', default=0.95, type=float,\n",
        "                        help='pseudo label threshold')\n",
        "    parser.add_argument('--k-img', default=65536, type=int,\n",
        "                        help='number of labeled examples')\n",
        "    parser.add_argument('--out', default='result',\n",
        "                        help='directory to output the result')\n",
        "    parser.add_argument('--resume', default='', type=str,\n",
        "                        help='path to latest checkpoint (default: none)')\n",
        "    parser.add_argument('--seed', type=int, default=-1,\n",
        "                        help=\"random seed (-1: don't use random seed)\")\n",
        "    parser.add_argument(\"--amp\", action=\"store_true\",\n",
        "                        help=\"use 16-bit (mixed) precision through NVIDIA apex AMP\")\n",
        "    parser.add_argument(\"--opt_level\", type=str, default=\"O1\",\n",
        "                        help=\"apex AMP optimization level selected in ['O0', 'O1', 'O2', and 'O3'].\"\n",
        "                        \"See details at https://nvidia.github.io/apex/amp.html\")\n",
        "    parser.add_argument(\"--local_rank\", type=int, default=-1,\n",
        "                        help=\"For distributed training: local_rank\")\n",
        "    parser.add_argument('--no-progress', action='store_true',\n",
        "                        help=\"don't use progress bar\")\n",
        "\n",
        "    args = parser.parse_args()\n",
        "    global best_acc\n",
        "\n",
        "    def create_model(args):\n",
        "        if args.arch == 'wideresnet':\n",
        "            import models.wideresnet as models\n",
        "            model = models.build_wideresnet(depth=args.model_depth,\n",
        "                                            widen_factor=args.model_width,\n",
        "                                            dropout=0,\n",
        "                                            num_classes=args.num_classes)\n",
        "        elif args.arch == 'resnext':\n",
        "            import models.resnext as models\n",
        "            model = models.build_resnext(cardinality=args.model_cardinality,\n",
        "                                         depth=args.model_depth,\n",
        "                                         width=args.model_width,\n",
        "                                         num_classes=args.num_classes)\n",
        "\n",
        "        logger.info(\"Total params: {:.2f}M\".format(\n",
        "            sum(p.numel() for p in model.parameters())/1e6))\n",
        "\n",
        "        return model\n",
        "\n",
        "    if args.local_rank == -1:\n",
        "        device = torch.device('cuda', args.gpu_id)\n",
        "        args.world_size = 1\n",
        "        args.n_gpu = torch.cuda.device_count()\n",
        "    else:\n",
        "        torch.cuda.set_device(args.local_rank)\n",
        "        device = torch.device('cuda', args.local_rank)\n",
        "        torch.distributed.init_process_group(backend='nccl')\n",
        "        args.world_size = torch.distributed.get_world_size()\n",
        "        args.n_gpu = 1\n",
        "\n",
        "    args.device = device\n",
        "\n",
        "    if args.dataset == 'cifar10':\n",
        "        args.num_classes = 10\n",
        "        if args.arch == 'wideresnet':\n",
        "            args.model_depth = 28\n",
        "            args.model_width = 2\n",
        "        if args.arch == 'resnext':\n",
        "            args.model_cardinality = 4\n",
        "            args.model_depth = 28\n",
        "            args.model_width = 4\n",
        "\n",
        "    elif args.dataset == 'cifar100':\n",
        "        args.num_classes = 100\n",
        "        if args.arch == 'wideresnet':\n",
        "            args.model_depth = 28\n",
        "            args.model_width = 10\n",
        "        if args.arch == 'resnext':\n",
        "            args.model_cardinality = 8\n",
        "            args.model_depth = 29\n",
        "            args.model_width = 64\n",
        "\n",
        "    logging.basicConfig(\n",
        "        format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n",
        "        datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
        "        level=logging.INFO if args.local_rank in [-1, 0] else logging.WARN)\n",
        "\n",
        "    logger.warning(\n",
        "        f\"Process rank: {args.local_rank}, \"\n",
        "        f\"device: {args.device}, \"\n",
        "        f\"n_gpu: {args.n_gpu}, \"\n",
        "        f\"distributed training: {bool(args.local_rank != -1)}, \"\n",
        "        f\"16-bits training: {args.amp}\",)\n",
        "\n",
        "    logger.info(dict(args._get_kwargs()))\n",
        "\n",
        "    if args.seed != -1:\n",
        "        set_seed(args)\n",
        "\n",
        "    if args.local_rank in [-1, 0]:\n",
        "        os.makedirs(args.out, exist_ok=True)\n",
        "        writer = SummaryWriter(args.out)\n",
        "\n",
        "    if args.local_rank not in [-1, 0]:\n",
        "        torch.distributed.barrier()\n",
        "\n",
        "    labeled_dataset, unlabeled_dataset, test_dataset = DATASET_GETTERS[args.dataset](\n",
        "        './data', args.num_labeled, args.k_img, args.k_img * args.mu)\n",
        "\n",
        "    model = create_model(args)\n",
        "\n",
        "    if args.local_rank == 0:\n",
        "        torch.distributed.barrier()\n",
        "\n",
        "    model.to(args.device)\n",
        "\n",
        "    train_sampler = RandomSampler if args.local_rank == -1 else DistributedSampler\n",
        "\n",
        "    labeled_trainloader = DataLoader(\n",
        "        labeled_dataset,\n",
        "        sampler=train_sampler(labeled_dataset),\n",
        "        batch_size=args.batch_size,\n",
        "        num_workers=args.num_workers,\n",
        "        drop_last=True)\n",
        "\n",
        "    unlabeled_trainloader = DataLoader(\n",
        "        unlabeled_dataset,\n",
        "        sampler=train_sampler(unlabeled_dataset),\n",
        "        batch_size=args.batch_size*args.mu,\n",
        "        num_workers=args.num_workers,\n",
        "        drop_last=True)\n",
        "\n",
        "    test_loader = DataLoader(\n",
        "        test_dataset,\n",
        "        sampler=SequentialSampler(test_dataset),\n",
        "        batch_size=args.batch_size,\n",
        "        num_workers=args.num_workers)\n",
        "\n",
        "    optimizer = optim.SGD(model.parameters(), lr=args.lr,\n",
        "                          momentum=0.9, nesterov=args.nesterov)\n",
        "\n",
        "    args.iteration = args.k_img // args.batch_size // args.world_size\n",
        "    args.total_steps = args.epochs * args.iteration\n",
        "    scheduler = get_cosine_schedule_with_warmup(\n",
        "        optimizer, args.warmup * args.iteration, args.total_steps)\n",
        "\n",
        "    if args.use_ema:\n",
        "        ema_model = ModelEMA(args, model, args.ema_decay, device)\n",
        "\n",
        "    start_epoch = 0\n",
        "\n",
        "    if args.resume:\n",
        "        logger.info(\"==> Resuming from checkpoint..\")\n",
        "        assert os.path.isfile(\n",
        "            args.resume), \"Error: no checkpoint directory found!\"\n",
        "        args.out = os.path.dirname(args.resume)\n",
        "        checkpoint = torch.load(args.resume)\n",
        "        best_acc = checkpoint['best_acc']\n",
        "        start_epoch = checkpoint['epoch']\n",
        "        model.load_state_dict(checkpoint['state_dict'])\n",
        "        if args.use_ema:\n",
        "            ema_model.ema.load_state_dict(checkpoint['ema_state_dict'])\n",
        "        optimizer.load_state_dict(checkpoint['optimizer'])\n",
        "        scheduler.load_state_dict(checkpoint['scheduler'])\n",
        "\n",
        "    if args.amp:\n",
        "        from apex import amp\n",
        "        model, optimizer = amp.initialize(\n",
        "            model, optimizer, opt_level=args.opt_level)\n",
        "\n",
        "    if args.local_rank != -1:\n",
        "        model = torch.nn.parallel.DistributedDataParallel(\n",
        "            model, device_ids=[args.local_rank],\n",
        "            output_device=args.local_rank, find_unused_parameters=True)\n",
        "\n",
        "    logger.info(\"***** Running training *****\")\n",
        "    logger.info(f\"  Task = {args.dataset}@{args.num_labeled}\")\n",
        "    logger.info(f\"  Num Epochs = {args.epochs}\")\n",
        "    logger.info(f\"  Batch size per GPU = {args.batch_size}\")\n",
        "    logger.info(\n",
        "        f\"  Total train batch size = {args.batch_size*args.world_size}\")\n",
        "    logger.info(f\"  Total optimization steps = {args.total_steps}\")\n",
        "\n",
        "    test_accs = []\n",
        "    model.zero_grad()\n",
        "    for epoch in range(start_epoch, args.epochs):\n",
        "\n",
        "        train_loss, train_loss_x, train_loss_u, mask_prob = train(\n",
        "            args, labeled_trainloader, unlabeled_trainloader,\n",
        "            model, optimizer, ema_model, scheduler, epoch)\n",
        "\n",
        "        if args.no_progress:\n",
        "            logger.info(\"Epoch {}. train_loss: {:.4f}. train_loss_x: {:.4f}. train_loss_u: {:.4f}.\"\n",
        "                        .format(epoch+1, train_loss, train_loss_x, train_loss_u))\n",
        "\n",
        "        if args.use_ema:\n",
        "            test_model = ema_model.ema\n",
        "        else:\n",
        "            test_model = model\n",
        "\n",
        "        test_loss, test_acc = test(args, test_loader, test_model, epoch)\n",
        "\n",
        "        if args.local_rank in [-1, 0]:\n",
        "            writer.add_scalar('train/1.train_loss', train_loss, epoch)\n",
        "            writer.add_scalar('train/2.train_loss_x', train_loss_x, epoch)\n",
        "            writer.add_scalar('train/3.train_loss_u', train_loss_u, epoch)\n",
        "            writer.add_scalar('train/4.mask', mask_prob, epoch)\n",
        "            writer.add_scalar('test/1.test_acc', test_acc, epoch)\n",
        "            writer.add_scalar('test/2.test_loss', test_loss, epoch)\n",
        "\n",
        "        is_best = test_acc > best_acc\n",
        "        best_acc = max(test_acc, best_acc)\n",
        "        if args.local_rank in [-1, 0]:\n",
        "            model_to_save = model.module if hasattr(model, \"module\") else model\n",
        "            if args.use_ema:\n",
        "                ema_to_save = ema_model.ema.module if hasattr(\n",
        "                    ema_model.ema, \"module\") else ema_model.ema\n",
        "            save_checkpoint({\n",
        "                'epoch': epoch + 1,\n",
        "                'state_dict': model_to_save.state_dict(),\n",
        "                'ema_state_dict': ema_to_save.state_dict() if args.use_ema else None,\n",
        "                'acc': test_acc,\n",
        "                'best_acc': best_acc,\n",
        "                'optimizer': optimizer.state_dict(),\n",
        "                'scheduler': scheduler.state_dict(),\n",
        "            }, is_best, args.out)\n",
        "\n",
        "        test_accs.append(test_acc)\n",
        "        logger.info('Best top-1 acc: {:.2f}'.format(best_acc))\n",
        "        logger.info('Mean top-1 acc: {:.2f}\\n'.format(\n",
        "            np.mean(test_accs[-20:])))\n",
        "\n",
        "    if args.local_rank in [-1, 0]:\n",
        "        writer.close()\n",
        "\n",
        "\n",
        "def train(args, labeled_trainloader, unlabeled_trainloader,\n",
        "          model, optimizer, ema_model, scheduler, epoch):\n",
        "    if args.amp:\n",
        "        from apex import amp\n",
        "    batch_time = AverageMeter()\n",
        "    data_time = AverageMeter()\n",
        "    losses = AverageMeter()\n",
        "    losses_x = AverageMeter()\n",
        "    losses_u = AverageMeter()\n",
        "    end = time.time()\n",
        "\n",
        "    if not args.no_progress:\n",
        "        p_bar = tqdm(range(args.iteration),\n",
        "                     disable=args.local_rank not in [-1, 0])\n",
        "\n",
        "    train_loader = zip(labeled_trainloader, unlabeled_trainloader)\n",
        "    model.train()\n",
        "    for batch_idx, (data_x, data_u) in enumerate(train_loader):\n",
        "        inputs_x, targets_x = data_x\n",
        "        (inputs_u_w, inputs_u_s), _ = data_u\n",
        "        data_time.update(time.time() - end)\n",
        "        batch_size = inputs_x.shape[0]\n",
        "        inputs = torch.cat((inputs_x, inputs_u_w, inputs_u_s)).to(args.device)\n",
        "        targets_x = targets_x.to(args.device)\n",
        "        logits = model(inputs)\n",
        "        logits_x = logits[:batch_size]\n",
        "        logits_u_w, logits_u_s = logits[batch_size:].chunk(2)\n",
        "        del logits\n",
        "\n",
        "        Lx = F.cross_entropy(logits_x, targets_x, reduction='mean')\n",
        "\n",
        "        pseudo_label = torch.softmax(logits_u_w.detach_(), dim=-1)\n",
        "        max_probs, targets_u = torch.max(pseudo_label, dim=-1)\n",
        "        mask = max_probs.ge(args.threshold).float()\n",
        "\n",
        "        Lu = (F.cross_entropy(logits_u_s, targets_u,\n",
        "                              reduction='none') * mask).mean()\n",
        "\n",
        "        loss = Lx + args.lambda_u * Lu\n",
        "\n",
        "        if args.amp:\n",
        "            with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
        "                scaled_loss.backward()\n",
        "        else:\n",
        "            loss.backward()\n",
        "\n",
        "        losses.update(loss.item())\n",
        "        losses_x.update(Lx.item())\n",
        "        losses_u.update(Lu.item())\n",
        "\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "        if args.use_ema:\n",
        "            ema_model.update(model)\n",
        "        model.zero_grad()\n",
        "\n",
        "        batch_time.update(time.time() - end)\n",
        "        end = time.time()\n",
        "        mask_prob = mask.mean().item()\n",
        "        if not args.no_progress:\n",
        "            p_bar.set_description(\"Train Epoch: {epoch}/{epochs:4}. Iter: {batch:4}/{iter:4}. LR: {lr:.6f}. Data: {data:.3f}s. Batch: {bt:.3f}s. Loss: {loss:.4f}. Loss_x: {loss_x:.4f}. Loss_u: {loss_u:.4f}. Mask: {mask:.4f}. \".format(\n",
        "                epoch=epoch + 1,\n",
        "                epochs=args.epochs,\n",
        "                batch=batch_idx + 1,\n",
        "                iter=args.iteration,\n",
        "                lr=scheduler.get_last_lr()[0],\n",
        "                data=data_time.avg,\n",
        "                bt=batch_time.avg,\n",
        "                loss=losses.avg,\n",
        "                loss_x=losses_x.avg,\n",
        "                loss_u=losses_u.avg,\n",
        "                mask=mask_prob))\n",
        "            p_bar.update()\n",
        "    if not args.no_progress:\n",
        "        p_bar.close()\n",
        "    return losses.avg, losses_x.avg, losses_u.avg, mask_prob\n",
        "\n",
        "\n",
        "def test(args, test_loader, model, epoch):\n",
        "    batch_time = AverageMeter()\n",
        "    data_time = AverageMeter()\n",
        "    losses = AverageMeter()\n",
        "    top1 = AverageMeter()\n",
        "    top5 = AverageMeter()\n",
        "    end = time.time()\n",
        "\n",
        "    if not args.no_progress:\n",
        "        test_loader = tqdm(test_loader,\n",
        "                           disable=args.local_rank not in [-1, 0])\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (inputs, targets) in enumerate(test_loader):\n",
        "            data_time.update(time.time() - end)\n",
        "            model.eval()\n",
        "\n",
        "            inputs = inputs.to(args.device)\n",
        "            targets = targets.to(args.device)\n",
        "            outputs = model(inputs)\n",
        "            loss = F.cross_entropy(outputs, targets)\n",
        "\n",
        "            prec1, prec5 = accuracy(outputs, targets, topk=(1, 5))\n",
        "            losses.update(loss.item(), inputs.shape[0])\n",
        "            top1.update(prec1.item(), inputs.shape[0])\n",
        "            top5.update(prec5.item(), inputs.shape[0])\n",
        "            batch_time.update(time.time() - end)\n",
        "            end = time.time()\n",
        "            if not args.no_progress:\n",
        "                test_loader.set_description(\"Test Iter: {batch:4}/{iter:4}. Data: {data:.3f}s. Batch: {bt:.3f}s. Loss: {loss:.4f}. top1: {top1:.2f}. top5: {top5:.2f}. \".format(\n",
        "                    batch=batch_idx + 1,\n",
        "                    iter=len(test_loader),\n",
        "                    data=data_time.avg,\n",
        "                    bt=batch_time.avg,\n",
        "                    loss=losses.avg,\n",
        "                    top1=top1.avg,\n",
        "                    top5=top5.avg,\n",
        "                ))\n",
        "        if not args.no_progress:\n",
        "            test_loader.close()\n",
        "\n",
        "    logger.info(\"top-1 acc: {:.2f}\".format(top1.avg))\n",
        "    logger.info(\"top-5 acc: {:.2f}\".format(top5.avg))\n",
        "    return losses.avg, top1.avg\n",
        "\n",
        "\n",
        "class ModelEMA(object):\n",
        "    def __init__(self, args, model, decay, device='', resume=''):\n",
        "        self.ema = deepcopy(model)\n",
        "        self.ema.eval()\n",
        "        self.decay = decay\n",
        "        self.device = device\n",
        "        self.wd = args.lr * args.wdecay\n",
        "        if device:\n",
        "            self.ema.to(device=device)\n",
        "        self.ema_has_module = hasattr(self.ema, 'module')\n",
        "        if resume:\n",
        "            self._load_checkpoint(resume)\n",
        "        for p in self.ema.parameters():\n",
        "            p.requires_grad_(False)\n",
        "\n",
        "    def _load_checkpoint(self, checkpoint_path):\n",
        "        checkpoint = torch.load(checkpoint_path)\n",
        "        assert isinstance(checkpoint, dict)\n",
        "        if 'ema_state_dict' in checkpoint:\n",
        "            new_state_dict = OrderedDict()\n",
        "            for k, v in checkpoint['ema_state_dict'].items():\n",
        "                if self.ema_has_module:\n",
        "                    name = 'module.' + k if not k.startswith('module') else k\n",
        "                else:\n",
        "                    name = k\n",
        "                new_state_dict[name] = v\n",
        "            self.ema.load_state_dict(new_state_dict)\n",
        "\n",
        "    def update(self, model):\n",
        "        needs_module = hasattr(model, 'module') and not self.ema_has_module\n",
        "        with torch.no_grad():\n",
        "            msd = model.state_dict()\n",
        "            for k, ema_v in self.ema.state_dict().items():\n",
        "                if needs_module:\n",
        "                    k = 'module.' + k\n",
        "                model_v = msd[k].detach()\n",
        "                if self.device:\n",
        "                    model_v = model_v.to(device=self.device)\n",
        "                ema_v.copy_(ema_v * self.decay + (1. - self.decay) * model_v)\n",
        "                # weight decay\n",
        "                if 'bn' not in k:\n",
        "                    msd[k] = msd[k] * (1. - self.wd)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    cudnn.benchmark = True\n",
        "    main()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EqnmbV2HnNrH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "python train.py --dataset cifar10 --num-labeled 4000 --arch wideresnet --batch-size 64 --lr 0.03 --out cifar10@4000"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}